{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mhS_Y6qgqW4Z",
        "wRb0HnWqVCDL",
        "hQg7l284V0oa",
        "qbGSRVPxjaT_",
        "cdoZKWxijmHk",
        "toHZ2o-Ljwwm",
        "QMbSmc52n-Ni",
        "hrXQ2d7OsmLl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMwkByBnaE49"
      },
      "source": [
        "<p style=\"text-align:center;\"><img src=\"data:image/jpeg;base64,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\" alt=\"EPITA Lab 1\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKtcL0CYn2n_"
      },
      "source": [
        "# Part 1. Keywords Extraction (14 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXcmrzodG1pt"
      },
      "source": [
        "## What is Keyword Extraction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZWKLXCYG9NJ"
      },
      "source": [
        "Keyword extraction is defined as the task that automatically identifies a set of the terms that best describe the subject of document. This is an important method in information retrieval (IR) systems: keywords simplify and speed up the search. Keyword extraction can be used to reduce the dimensionality of text for further text analysis (text classification ot topic modeling). S.Art et al., for example, extracted keywords to measure patent similarity. Using keyword extraction, you can automatically index data, summarize a text, or generate tag clouds with the most representative keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O3FI910HETW"
      },
      "source": [
        "## How to extract the keywords?\n",
        "All keyword extraction algorithms include the following steps:\n",
        "\n",
        "* Candidate generation. Detection of possible candidate keywords from the text.\n",
        "* Property calculation. Computation of properties and statistics required for ranking.\n",
        "* Ranking. Computation of a score for each candidate keyword and sorting in descending order of all candidates. The top n candidates are finally selected as the n keywords representing the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD64EYyAHE5k"
      },
      "source": [
        "# all the imports \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3WHRR0C9Il"
      },
      "source": [
        "## Goal.\n",
        "\n",
        "In the following, given a paper, we will extract the keywords associated to this paper. Each individual can have their own qualitative assessment of what is \"key\" word. However, we will try as much as possible to objectify the approach and quantify to what extent a keyword is indeed key to the paper in question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUQe5Df4Bpr"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT3cUUST9QX6"
      },
      "source": [
        "%%capture\n",
        "! git clone https://github.com/MastafaF/ExtractKeywords.git"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJTpCLQ9XJ5",
        "outputId": "4d4072d2-0c08-49c2-9411-1184faaa9b0f"
      },
      "source": [
        "import os \n",
        "\n",
        "os.listdir(\"./ExtractKeywords\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', '.git', 'LICENSE', 'data.tar.gz', 'README.md']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVjwTfE9dyP",
        "outputId": "d9e37d15-767c-40d4-a464-e277a75ca4aa"
      },
      "source": [
        "# Extract data file \n",
        "\n",
        "! cd ExtractKeywords && tar -zxvf data.tar.gz data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/papers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "CdRVtKw44C9p",
        "outputId": "67f55602-1ff2-4694-a3f3-517d296bb1ff"
      },
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('./ExtractKeywords/data/papers.csv')\n",
        "df.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-335f3de9-5f30-4c15-ab6f-67280a0bd8dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-335f3de9-5f30-4c15-ab6f-67280a0bd8dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-335f3de9-5f30-4c15-ab6f-67280a0bd8dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-335f3de9-5f30-4c15-ab6f-67280a0bd8dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IElpuJ76o0"
      },
      "source": [
        "## Preprocessing data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRAj0JXjAWBo",
        "outputId": "f7586fb7-9c47-4eb9-de74-866bad2ae710"
      },
      "source": [
        "# For the Lemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHC9ShM-IX7E"
      },
      "source": [
        "### Question 1.1: Preprocessing data in a meaningful way [code] (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWpI1OJk78Gc"
      },
      "source": [
        "import re\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "\n",
        "# Me \n",
        "from gensim.parsing import preprocess_string, strip_short, strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
        "\n",
        "# Update stop words accordingly\n",
        "#my_stop_words = STOPWORDS.union(set(['mystopword1', 'mystopword2']))\n",
        "my_stop_words = STOPWORDS.union(set(['\\n', '~\\n\\n']))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "\n",
        "stop_words = STOPWORDS.union(set(new_words))\n",
        "\n",
        "def pre_process(text):\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  CUSTOM_FILTERS = [\n",
        "    lambda s: s.lower(),\n",
        "    lambda s: re.sub(r'\\s+\\w{1}\\s+', ' ', s),\n",
        "    strip_tags,\n",
        "    strip_numeric,\n",
        "    strip_punctuation, \n",
        "    strip_multiple_whitespaces,\n",
        "    strip_short,\n",
        "    #remove_stopwords, # Removes all English generic stopwords\n",
        "  ]\n",
        "\n",
        "  text = preprocess_string(text, CUSTOM_FILTERS)\n",
        "\n",
        "  lemmatized_text = []\n",
        "  wnl = WordNetLemmatizer()\n",
        "  for word in text:\n",
        "    lemmatized_text.append(wnl.lemmatize(word))\n",
        "\n",
        "  text_wo_stopwords = [word for word in lemmatized_text if not word in stop_words]\n",
        "  final_text = ' '.join(text_wo_stopwords)\n",
        "  \n",
        "  return final_text\n",
        "\n",
        "  # ------------------"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZFsz9JX_9I0",
        "outputId": "e277c0b6-23ae-41b4-e644-3c3cae94aaf4"
      },
      "source": [
        "%%time\n",
        "df['preproc_text'] = df['paper_text'].apply(pre_process)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 3s, sys: 376 ms, total: 2min 4s\n",
            "Wall time: 2min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_3ADgBBsAHhg",
        "outputId": "100ffb5b-595c-4aab-ee10-0d478b89d0a6"
      },
      "source": [
        "# Visualizing data \n",
        "HTML(pd.DataFrame(df.loc[0, [\"preproc_text\"]]).to_html())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>preproc_text</th>\n",
              "      <td>self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed application robot eyesight proposed database associate input output half discussion algorithm self organization proposed aspect hardware produce new style neural network half applicability handwritten letter recognition autonomous mobile robot demonstrated introduction let mapping given finite infinite set finite infinite set learning machine observes set pair sampled randomly mean cartesian product computes estimate small estimation error measure usually faster decrease estimation error increase number better learning machine expression performance incomplete lack consideration candidate assumed preliminarily good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating parameter indefinite structure equivalent define implicitly set candidate subset mapping computes value parameter based observed type parameter type learning machine defined approach number increase alternative case estimation error remains eternally problem designing learning machine return proper structure sense hand assumed structure demanded compact possible achieve fast learning word number parameter small parameter uniquely determined observed demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge given unknown case comparatively easy proper compact structure alternative case difficult possible solution compactness assume almighty structure cover combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed discussion flourishing efficient method proposed recently hardware unit computing coefficient parallel speed sold anza mark iii odyssey neural network exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose located near case estimation error negligible distant estimation error negligible research report following situation appears complex estimation error converges value number increase decrease hardly dimension heighten property considerable defect neural network recursi type recursive type founded methodology learning follows initial stage set instead notation candidate equal set mapping observing reduced observing second reduced candidate set gradually small observation proceeds observing write likelihood estimation selected contrarily parameter type recursive type guarantee surely approach number increase recursive type observes rewrite value correlated type ha architecture composed rule rewriting free memory space architecture form naturally kind database build management data self organizing way database differs ordinary following sense doe record observed computes estimation database associative database subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari dissimilari mean mapping real necessarily defined single formula definable example collection rule written form dissimilarity defines structure locally knowledge imperfect flect heuristic way contrarily neural network possible accelerate speed learning establishing especially easily simple process analogically information like human application paper recursive type strongly effectiveness denote sequence observed simplest construction associative database observing follows algorithm initial stage let set let equal min furthermore add produce version improved economize memory follows algorithm initial stage let composed arbitrary element let lex equal min furthermore let add produce construction approach increase computation time grows proportionally size second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence algorithm constructing associative database follows algorithm step initialization let root root variable assigned respective node memorize data furthermore let step increase reset pointer root repeat following arrives terminal node leaf notation nand let mean descendant node let step display yin related information yin step establish new descendant node secondly let yin yin yin finally step loop step stopped time continued suppose gate element artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letter elastic matching method recognizing hand written discrete english letter global training fuzzy logic search method recognizing chinese character written square style published self organization associative database realizes recognition handwritten continuous english letter nov dwlo source document loo windowing number nualber sampl experiment scanner document letter recognizer parallelogram window cover maximal letter process sequence letter shifting window recognizer scan word slant direction place window left vicinity black point detected window catch letter succeeding letter recognition head letter performed end position boundary line letter known starting scanning boundary repeating operation recognizer accomplishes recursively task major problem come identifying head letter window considering define following regard window define accordingly denote black point left area boundary window project window measure euclidean distance black point closest let summation black point divided number regard couple reading position boundary define accordingly operator teach recognizer interaction relation window reading boundary algorithm precisely recalled reading incorrect operator teach correct reading console boundary position incorrect teach correct position mouse partially document experiment change number node recognition rate defined relative frequency correct answer past trial speciiications window height dot width dot slant angular deg example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case doe attain distinguishable excessive lluctuation writing consistency relation assured like number node increase endlessly clever stop learning recognition rate attains upper limit improve recognition rate consider spelling word future subject obstacle avoiding movement camera type autonomous mobile robot reported flourishingly author belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator motion robot learning natural length width height robot weight visual angle camera deg robot ha following factor motion turn le deg advance le control speed le experiment wa passageway wid inside building author laboratory exist experimental intention arrange box smoking stand gas cylinder stool handcart passage way random let robot camera recall similar trace route preliminarily recorded purpose define following let camera face deg downward process low pas filter scanning vertically filtered search point luminance change excessively bstitu point white point black obstacle exists robot white area free area robot regard binary dot processed define accordingly let number black point exclusive regard obtained drawing route define accordingly robot superimposes current camera route recalled inquires operator instruction operator judge subjectively suggested route appropriate negative answer draw desirable route mouse teach new robot opera tion defines implicitly sequence reflecting cost criterion operator iibube roan stationary uni configuration autonomous mobile robot north rmbi unit robot roan experimental environment wall camera preprocessing preprocessing course suggest ion search processing obstacle avoiding movement processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency attains time notice rest doe mean directly percentage collision practice prevent collision adopting supplementary measure time number node wa level tree distributed proposed method reflects delicately character operator example robot trained operator slowly space obstacle trained operator brush quickly obstacle fact hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera position data respectively memory capacity finite actual compu ters compress camera slight loss information compression admittable long precision position identification acceptable area major problem come suitable compression method experimental environment jut passageway interval section adjacent jut ha door robot identifies roughly surrounding landscape section place temporarily triangular surveying technique exact measure necessary realize task define following turn camera panorama deg scanning horizontally center line substitute point luminance excessively change black point white regard binary dot line processed define accordingly project black point measure euclidean distance black point closest let summation similarly calculate exchanging role denoting number respectively nand define regard positive integer labeled section define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway area learns relation landscape position data position identification area achieved crossing plural database task automatic excepting periodic reset counter kind learning teacher define identification rate relative frequency correct recall position data past trial typical example converged time time number level wa level oftree distributed identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera loosened possibility depends improvement hardware future example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving time interval frame sec actual motion robot conclusion method self organizing associative database wa proposed application robot eyesight machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced subclass structure subject imposed widen class probable solution abolish addressing rule depending directly value instead establish rule depending distribution function value investigation reference hopfield tank computing neural circuit model science rumelhart learning representation propagating error nature hull hypothesis generation computational model visual word recognition ieee expert fall kurtzberg feature analysis symbol recognition elastic matching ibm develop wang suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami brook self calibration motion stereo vision mobile robot int symp robotics research goto stentz cmu mobile robot navigation ieee int conf robotics automation madarasz design autonomous vehicle disabled ieee jour robotics automation triendl kriegman stereo vision navigation building ieee int conf robotics automation turk video road following autonomous land vehicle ieee int conf robotics automation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLS9mbe8B4UY"
      },
      "source": [
        "## 0. Raw counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WY9c6iwI0Lv"
      },
      "source": [
        "### Question 1.2: Build a top N words based on occurence [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izLhCAH5B-jy"
      },
      "source": [
        "\"\"\"\n",
        "Idea: \n",
        "\n",
        "0. Split with spacy OR nltk \n",
        "\n",
        "1. Counter \n",
        "\n",
        "2. Surface top 10 \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_counter(txt_preproc, N=10): \n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    tokens = nltk.word_tokenize(txt_preproc)\n",
        "    count_tokens = nltk.FreqDist(tokens)\n",
        "\n",
        "    count_tokens_list = list(dict(count_tokens).items())\n",
        "    count_tokens_list.sort(key=lambda ele: ele[1], reverse=True)\n",
        "\n",
        "    return count_tokens_list[:10]\n",
        "    # ------------------\n",
        "\n",
        "df[\"Top N\"] = df[\"preproc_text\"].apply(get_counter)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "Xt7vPHWTaDCX",
        "outputId": "35e02df4-8587-4d58-c3ee-616c49173659"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "7236   994  1994                Single Transistor Learning Synapses   \n",
              "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
              "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
              "7239   998  1994  Learning direction in global motion: two class...   \n",
              "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
              "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
              "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
              "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
              "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
              "\n",
              "              abstract                                         paper_text  \\\n",
              "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...   \n",
              "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...   \n",
              "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...   \n",
              "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...   \n",
              "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...   \n",
              "...                ...                                                ...   \n",
              "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...   \n",
              "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...   \n",
              "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....   \n",
              "7239  Abstract Missing  Learning direction in global motion: two\\nclas...   \n",
              "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "0     self organization associative database applica...   \n",
              "1     mean field theory layer visual cortex applicat...   \n",
              "2     storing covariance associative long term poten...   \n",
              "3     bayesian query construction neural network mod...   \n",
              "4     neural network ensemble cross validation activ...   \n",
              "...                                                 ...   \n",
              "7236  single transistor learning synapsis paul hasle...   \n",
              "7237  bias variance combination square estimator ron...   \n",
              "7238  real time clustering cmos neural engine serran...   \n",
              "7239  learning direction global motion class psychop...   \n",
              "7240  correlation interpolation network real time ex...   \n",
              "\n",
              "                                                  Top N  \n",
              "0     [(robot, 23), (database, 19), (let, 18), (lear...  \n",
              "1     [(cell, 51), (network, 42), (cortical, 32), (s...  \n",
              "2     [(input, 58), (weak, 42), (synaptic, 36), (ass...  \n",
              "3     [(loss, 42), (query, 25), (model, 23), (functi...  \n",
              "4     [(ensemble, 56), (network, 52), (error, 51), (...  \n",
              "...                                                 ...  \n",
              "7236  [(gate, 44), (weight, 42), (synapse, 42), (cur...  \n",
              "7237  [(estimator, 37), (variance, 29), (bias, 25), ...  \n",
              "7238  [(current, 35), (chip, 21), (input, 19), (patt...  \n",
              "7239  [(learning, 69), (motion, 42), (direction, 32)...  \n",
              "7240  [(model, 54), (view, 47), (expression, 46), (f...  \n",
              "\n",
              "[7241 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8060774c-cc44-4a2b-9bb8-868cd7ddab04\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "      <td>self organization associative database applica...</td>\n",
              "      <td>[(robot, 23), (database, 19), (let, 18), (lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "      <td>mean field theory layer visual cortex applicat...</td>\n",
              "      <td>[(cell, 51), (network, 42), (cortical, 32), (s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "      <td>storing covariance associative long term poten...</td>\n",
              "      <td>[(input, 58), (weak, 42), (synaptic, 36), (ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "      <td>bayesian query construction neural network mod...</td>\n",
              "      <td>[(loss, 42), (query, 25), (model, 23), (functi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "      <td>neural network ensemble cross validation activ...</td>\n",
              "      <td>[(ensemble, 56), (network, 52), (error, 51), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>994</td>\n",
              "      <td>1994</td>\n",
              "      <td>Single Transistor Learning Synapses</td>\n",
              "      <td>NaN</td>\n",
              "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "      <td>single transistor learning synapsis paul hasle...</td>\n",
              "      <td>[(gate, 44), (weight, 42), (synapse, 42), (cur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>996</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "      <td>bias variance combination square estimator ron...</td>\n",
              "      <td>[(estimator, 37), (variance, 29), (bias, 25), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>997</td>\n",
              "      <td>1994</td>\n",
              "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
              "      <td>NaN</td>\n",
              "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "      <td>real time clustering cmos neural engine serran...</td>\n",
              "      <td>[(current, 35), (chip, 21), (input, 19), (patt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>998</td>\n",
              "      <td>1994</td>\n",
              "      <td>Learning direction in global motion: two class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "      <td>learning direction global motion class psychop...</td>\n",
              "      <td>[(learning, 69), (motion, 42), (direction, 32)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>999</td>\n",
              "      <td>1994</td>\n",
              "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "      <td>correlation interpolation network real time ex...</td>\n",
              "      <td>[(model, 54), (view, 47), (expression, 46), (f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows  9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8060774c-cc44-4a2b-9bb8-868cd7ddab04')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8060774c-cc44-4a2b-9bb8-868cd7ddab04 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8060774c-cc44-4a2b-9bb8-868cd7ddab04');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-J0RHMMFW6b",
        "outputId": "365184a2-3fcb-4732-d169-ca8424b6e457"
      },
      "source": [
        "df.loc[2, \"Top N\"]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('input', 58),\n",
              " ('weak', 42),\n",
              " ('synaptic', 36),\n",
              " ('associative', 35),\n",
              " ('ltp', 30),\n",
              " ('strong', 26),\n",
              " ('phase', 26),\n",
              " ('long', 24),\n",
              " ('stimulus', 23),\n",
              " ('hippocampus', 22)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7YbNCyJaqk"
      },
      "source": [
        "### Question 1.3: What are some of the limits of raw counts? How could we improve the approach through preprocessing? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eewWd_TjJlt-"
      },
      "source": [
        "Counting the number of times each word appears in a document is a very simple approach to text processing, but it has several limitations. First, it does not account for the order of the words in the document, so two documents with the same words in different orders will be considered to be identical. Second, it does not account for different forms of the same word, so \"fish\" and \"fishing\" will be considered to be different words. Third, raw counts could be very low for rare words, and this could also skew the similarity results. Finally, it does not account for the context of the words in the document, so two documents with the same words but different meanings will be considered to be identical.\n",
        "\n",
        "One way to improve the approach is to use a technique called \"stemming\" to reduce each word to its base form before counting. This will account for different forms of the same word. Further, we could use a weighting scheme such as tf-idf. This will palliate rare words getting low scores and skewing the results. Another way to improve the approach is to use a technique called \"stopword removal\" to remove common words such as \"a\", \"the\", and \"of\" before counting. This will account for the context of the words in the document.\n",
        "\n",
        "We've implemented in question 1.1 both the removal of stop words and the lemmatization which is the process of grouping together the different forms of a word so they can be analyzed as a single item."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wao4NivXGPIM"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRcD4MeHFnt"
      },
      "source": [
        "### Introduction.\n",
        "\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZe8obbHm-Q"
      },
      "source": [
        "### CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pd54cOkGV_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079dd95a-4437-41b7-96c8-f257a5f7943a"
      },
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "\n",
        "\n",
        "word_count_vector=cv.fit_transform(df[\"preproc_text\"])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 25s, sys: 6.06 s, total: 2min 31s\n",
            "Wall time: 2min 31s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fngZVCVGcG_",
        "outputId": "4297dfe1-2036-44be-fbe3-48d4b39796ca"
      },
      "source": [
        "word_count_vector"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 5316905 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxV_evcXHuKp"
      },
      "source": [
        "### TfidfTransformer to Compute Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfwFzHCHzOY",
        "outputId": "fd68a418-972e-458d-f050-31dfd34639d5"
      },
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,\n",
        "                                   use_idf=True)\n",
        "\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 29.9 ms, sys: 1.98 ms, total: 31.9 ms\n",
            "Wall time: 33.9 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd9hXGWJHzLy",
        "outputId": "40254ec1-e0e6-4b8a-c710-d18f40ad7bc9"
      },
      "source": [
        "tfidf_transformer"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1wbWizrJ5CE"
      },
      "source": [
        "### Question 1.4: How can you find an optimal max_df? Why are we using a sparse matrix instead of a regular matrix? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EekhsGbKYoyv"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_9miFxKaU1",
        "outputId": "afbe0bc0-8489-460e-f2b3-8a9dbfce6683"
      },
      "source": [
        "cv.transform([\" change number node recognition rate defined relative frequency\"])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "K2Vdn9NTMIxI",
        "outputId": "88113283-af63-4455-eaff-58d65d3bb80f"
      },
      "source": [
        "# Visualizing data \n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "plt.spy(csr_matrix(cv.transform([\"change number node recognition rate defined relative frequency\"])))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f76df799590>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRI_AZtROxP0",
        "outputId": "e8ac27b0-8a00-4d1e-d9d7-207fe349685d"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([\"change number node recognition rate defined relative frequency\"]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "sorted_items"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7360, 0.6332528462762741),\n",
              " (6110, 0.48592082711420476),\n",
              " (3282, 0.2823916239785495),\n",
              " (5971, 0.2677155552680803),\n",
              " (7348, 0.2262665323411533),\n",
              " (7490, 0.2242783939550881),\n",
              " (1179, 0.1963759835498492),\n",
              " (7270, 0.17762842408345383),\n",
              " (2062, 0.15648471115689686),\n",
              " (6087, 0.12391506848540515)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxhq2LiEOxD3",
        "outputId": "a9e81741-930c-4db2-8957-83bfa1dbd6e9"
      },
      "source": [
        "coo_matrix = tf_idf_vector.tocoo()\n",
        "# list(zip(coo_matrix.col, coo_matrix.data))\n",
        "coo_matrix"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4EwdqH_Gm0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04c4435-3120-452c-c49b-3d17cb107f68"
      },
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(txt, top_N=10):\n",
        "\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "\n",
        "  # ------------------\n",
        "  \n",
        "  return sorted_keyword_score"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ahvTo9gjSWCW",
        "outputId": "387acdab-9ccb-4e21-c1aa-fef0898a40c1"
      },
      "source": [
        "get_keywords(txt=\"change number node recognition rate defined relative frequency\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0ed90c8feb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"change number node recognition rate defined relative frequency\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-7381c5a98070>\u001b[0m in \u001b[0;36mget_keywords\u001b[0;34m(txt, top_N)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# ------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted_keyword_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sorted_keyword_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9hzsg0CUQZ_"
      },
      "source": [
        "### Compare Raw Counts to Tf-IDF approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot6GLVIiUPqj"
      },
      "source": [
        "df[\"Top_N_TF-IDF\"] = df[\"preproc_text\"].apply(get_keywords, top_N=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4sydV20UjvA"
      },
      "source": [
        "df.sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDLV8yILNNS"
      },
      "source": [
        "### Question 1.5: Find an example where there is a noticeable difference between tf-idf and raw counts? Justify which method you would choose yourself (there is no bad and good answer here) [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mes_RnLNVXBX"
      },
      "source": [
        "## 2. KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7iWThDYXlJY"
      },
      "source": [
        "## 2.0. Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Ji149nXmUU"
      },
      "source": [
        "%%capture\n",
        "pip install keybert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-kXWUGKXqIB"
      },
      "source": [
        "%%capture\n",
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of learning a function that\n",
        "         maps an input to an output based on example input-output pairs. It infers a\n",
        "         function from labeled training data consisting of a set of training examples.\n",
        "         In supervised learning, each example is a pair consisting of an input object\n",
        "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
        "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
        "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
        "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
        "         the learning algorithm to generalize from the training data to unseen situations in a \n",
        "         'reasonable' way (see inductive bias).\n",
        "      \"\"\"\n",
        "kw_model = KeyBERT()\n",
        "keywords = kw_model.extract_keywords(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNrYae2jYDqU"
      },
      "source": [
        "keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJs_f30AYmKN"
      },
      "source": [
        "### Question 2.0. Apply KeyBERT to the a sample of the dataset [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GisGAL0pIDy"
      },
      "source": [
        "df_ = df.sample(100)\n",
        "df_.sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN3Dlu11YwLE"
      },
      "source": [
        "%%time\n",
        "%%capture\n",
        "\n",
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYDNwWa5ZiTK"
      },
      "source": [
        "# TODO: compare the same paper example across the 3 methods \n",
        "\n",
        "idx_focus = 121 \n",
        "\n",
        "df_.loc[121, \"Top_N_KeyBERT_1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GQQGMTZgrR"
      },
      "source": [
        "### Question 2.2. Comparison of multilple techniques [written] (4 points)\n",
        "\n",
        "1. Draw a table of the solution, the quality score that you defined and the time taken to find keywords across a sample of 1000 of the original dataset. \n",
        "2. Can you think of tweaks to reduce time to compute? If yes, add an additional column to the above table with your proposed tweaks.\n",
        "3. Based on the above table and  lecture 1, what do you think is the most appropriate solution for keywords extraction? Why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhS_Y6qgqW4Z"
      },
      "source": [
        "# Part 2. Word Vectors (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kedMB9f-qsnw"
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5S4yJ_ZrHAo"
      },
      "source": [
        "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
        "\n",
        "Note on Terminology: The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcD1SUIrP_m"
      },
      "source": [
        "## Count-Based Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvm6lbSsFD0"
      },
      "source": [
        "Most word vector models start from the following idea:\n",
        "\n",
        "You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
        "\n",
        "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMxbozcslLA"
      },
      "source": [
        "## Plotting Co-Occurrence Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OO_oowsrK2"
      },
      "source": [
        "\n",
        "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a read_corpus function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds <START> and <END> tokens to each of the documents, and lowercases words. You do not have to perform any other kind of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTQwympsqDq"
      },
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrwL93ns1Qy"
      },
      "source": [
        "Let's have a look what these documents are like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eZvFI3Qs0x4"
      },
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNKy6j3as7xJ"
      },
      "source": [
        "### Question 2.1: Implement distinct_words [code] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIgkQ47otdqZ"
      },
      "source": [
        "Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with for loops, but it's more efficient to do it with Python list comprehensions. In particular, this may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's more information.\n",
        "\n",
        "Your returned corpus_words should be sorted. You can use python's sorted function for this.\n",
        "\n",
        "You may find it useful to use Python sets to remove duplicate words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTIH5vFetgjD"
      },
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents - eg [[\"hey\", \"I\", \"am\", \"toto\"], [\"hey\", \"I\", \"am\", \"tata\"]]\n",
        "        Return:\n",
        "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.w\n",
        "\n",
        "    # ------------------\n",
        "\n",
        "    return corpus_words, num_corpus_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZX4dH8stmYN"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fD2hYr3fw8"
      },
      "source": [
        "### Question 2.2: Implement compute_co_occurrence_matrix [code] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4MLCIa3lKw"
      },
      "source": [
        "Write a method that constructs a co-occurrence matrix for a certain window-size  n  (with a default of 4), considering words  n  before and  n  after the word in the center of the window. Here, we start to use numpy (np) to represent vectors, matrices, and tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz5vrGb43lbA"
      },
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter \n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2ind = {}\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    return M, word2ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guUdCsM2BUuC"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCC24T0WPyI2"
      },
      "source": [
        "### Question 2.3: Implement reduce_to_k_dim [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9XXG-WP2dZ"
      },
      "source": [
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use sklearn.decomposition.TruncatedSVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jfqvOUOP8R6"
      },
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGeaWNuRAnJ"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iTgMaquRQKB"
      },
      "source": [
        "### Question 2.4: Implement plot_embeddings [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H629WACPRTg2"
      },
      "source": [
        "Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).\n",
        "\n",
        "For this example, you may find it useful to adapt this code. In the future, a good way to make a plot is to look at the Matplotlib gallery, find a plot that looks somewhat like what you want, and adapt the code they give."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMfaxKfERT1P"
      },
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ5sOXmXRYOa"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below. \n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRb0HnWqVCDL"
      },
      "source": [
        "### Question 2.5: Co-Occurrence Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfAWdLFVFx-"
      },
      "source": [
        "Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" (oil) corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). Note: The line of code below that does the normalizing uses the NumPy concept of broadcasting. If you don't know about broadcasting, check out Computation on Arrays: Broadcasting by Jake VanderPlas.\n",
        "\n",
        "Run the below cell to produce the plot. It'll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? Note: \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3d0UK2iVFDM"
      },
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQg7l284V0oa"
      },
      "source": [
        "# Part 3. Prediction-based word vectors (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNq-K6UzzDP"
      },
      "source": [
        "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). If you're feeling adventurous, challenge yourself and try reading GloVe's original paper.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory. Note: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqq7A2IWz011"
      },
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
        "    return wv_from_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYbJ59Jiz7OA"
      },
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take a couple minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edzctdyh0rDm"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbGSRVPxjaT_"
      },
      "source": [
        "## Reducing dimensionality of Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXr1zGXSjddn"
      },
      "source": [
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead. Run the following cells to:\n",
        "\n",
        "Put 10000 Glove vectors into a matrix M\n",
        "Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_xj1ApzjfOr"
      },
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']):\n",
        "    \"\"\" Put the GloVe vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 200) containing the vectors\n",
        "            word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(224)\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        if w in words:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHVTZLiBjhN5"
      },
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoZKWxijmHk"
      },
      "source": [
        "### Question 3.1: GloVe Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDIugqjjo3R"
      },
      "source": [
        "Run the cell below to plot the 2D GloVe embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK438bCgjm98"
      },
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "plot_embeddings(M_reduced_normalized, word2ind, words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cCl0DQjuJN"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHZ2o-Ljwwm"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBZbtLYzj1R_"
      },
      "source": [
        "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
        "\n",
        "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLE4DOTNj69N"
      },
      "source": [
        "### Question 3.2: Words with Multiple Meanings (1.5 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LT0bH4kolN"
      },
      "source": [
        "Polysemes and homonyms are words that have more than one meaning (see this wiki page to learn more about the difference between polysemes and homonyms ). Find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain one of the meanings of the words)?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the GenSim documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgoE4V3rj76o"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z90e-p_jktYD"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBt1A0Y8kyqx"
      },
      "source": [
        "### Question 3.3: Synonyms & Antonyms (2 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gWr_Cvk3Tu"
      },
      "source": [
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words  (w1,w2,w3)  where  w1  and  w2  are synonyms and  w1  and  w3  are antonyms, but Cosine Distance  (w1,w3)<  Cosine Distance  (w1,w2) .\n",
        "\n",
        "As an example,  w1 =\"happy\" is closer to  w3 =\"sad\" than to  w2 =\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwHp4noktJF"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AD2asvrk7Y7"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0uNtlXZlAdy"
      },
      "source": [
        "### Question 3.4: Analogies with Word Vectors [written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKCiSDhlEBf"
      },
      "source": [
        "Word vectors have been shown to sometimes exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the most_similar function from the GenSim documentation. The function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list (while omitting the input words, which are often the most similar; see this paper). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHlsY4kolA6f"
      },
      "source": [
        "# Run this cell to answer the analogy -- man : king :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv1gqPl5nQ4H"
      },
      "source": [
        "Let  m ,  k ,  w , and  x  denote the word vectors for man, king, woman, and the answer, respectively. Using only vectors  m ,  k ,  w , and the vector arithmetic operators  +  and    in your answer, what is the expression in which we are maximizing cosine similarity with  x ?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would man and woman lie in the coordinate plane relative to king and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zvj-YaOnTAY"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzhOOfnuql"
      },
      "source": [
        "### Question 3.5: Finding Analogies [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5xFEsKVnzT0"
      },
      "source": [
        "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "Note: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrWfGGznRMH"
      },
      "source": [
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygUqza7n8mn"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbSmc52n-Ni"
      },
      "source": [
        "### Question 3.6: Incorrect Analogy [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKhHs5uooAaD"
      },
      "source": [
        "Find an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayAfn_MPnzrF"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqq_EaXoDYl"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBlWGAmoGCy"
      },
      "source": [
        "### Question 3.7: Guided Analysis of Bias in Word Vectors [written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_rVPQteoINq"
      },
      "source": [
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDlvsts2oBsp"
      },
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
        "# most dissimilar from.\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtOxfydoLZb"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMsfYJyxoNWT"
      },
      "source": [
        "###  Question 3.8: Independent Analysis of Bias in Word Vectors [code + written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42ajQDcodeV"
      },
      "source": [
        "Use the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9d5cbx3oJsf"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBxYCeEolk5"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNaetQ4donRi"
      },
      "source": [
        "### Question 3.9: Thinking About Bias [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mktU3tdqtzp"
      },
      "source": [
        "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?\n",
        "\n",
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXQ2d7OsmLl"
      },
      "source": [
        "# Part 4. Prediction-based sentence vectors (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOP9j0mV2rvU"
      },
      "source": [
        "Sentence embeddings are a more powerful representation than word embeddings. They allow you to have out-of-the-box sentence representation of sequences of tokens which is closer to what you would have in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3pVRQ8wE4P"
      },
      "source": [
        "### Question 4.1: How would you represent a sentence with Glove? What are the limits of your proposed implementation? [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1w30iYj0YJn"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2n3vp8g3AEe"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9F_tbMr3CZT"
      },
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBTJnO606Tpc"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnidR9Gg2697"
      },
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load SentenceBERT Vectors\n",
        "        Return:\n",
        "            embedder: sentence embedder \n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return embedder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzL9oBS4Zoc"
      },
      "source": [
        "%%capture\n",
        "embedder = load_embedding_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKkFzRHe6bEm"
      },
      "source": [
        "Inspired by the above, choose the appropriate way to plot the below clusters. Do they make sense to you? What would you improve to get a meaningful plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-o3T8Wz-Feb"
      },
      "source": [
        "### Question 4.2. Evaluate clustering quality of SentenceBERT. What makes it good at clustering sentences? Which method of the two below would you go for? [written] (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqHOmWtqyNo3"
      },
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Perform kmean clustering\n",
        "num_clusters = 5\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4wF5uTy50Nt"
      },
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44mDLKmVyOUu"
      },
      "source": [
        "### Question 4.3: SentenceBERT Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ykELvWt7ZX"
      },
      "source": [
        "Plot the above corpus with your favorite method in a 2-dimensional space. Comment on the output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqSyMyy-bm8"
      },
      "source": [
        "### Question 4.4: Independent Analysis of Bias in Word Vectors [code + written] (4 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZynvZnq-nqk"
      },
      "source": [
        "Select a corpus of interest, or examples of interest and shed light on one source of bias from SentenceBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQyOEVUWvEmI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}