{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rKtcL0CYn2n_",
        "eXcmrzodG1pt",
        "1O3FI910HETW",
        "jr3WHRR0C9Il",
        "XoUQe5Df4Bpr",
        "C2IElpuJ76o0",
        "BHC9ShM-IX7E",
        "lLS9mbe8B4UY",
        "9WY9c6iwI0Lv",
        "KK7YbNCyJaqk",
        "Wao4NivXGPIM",
        "5TRcD4MeHFnt",
        "QOZe8obbHm-Q",
        "nxV_evcXHuKp",
        "A1wbWizrJ5CE",
        "x9hzsg0CUQZ_",
        "qkDLV8yILNNS",
        "Mes_RnLNVXBX",
        "t7iWThDYXlJY",
        "vJs_f30AYmKN",
        "h9GQQGMTZgrR",
        "ObtV0t6W5Wqe",
        "mhS_Y6qgqW4Z",
        "HKcD1SUIrP_m",
        "4vMxbozcslLA",
        "bNKy6j3as7xJ",
        "86fD2hYr3fw8",
        "yCC24T0WPyI2",
        "8iTgMaquRQKB",
        "wRb0HnWqVCDL",
        "4HVTaEqKRDck",
        "edzctdyh0rDm",
        "b2n3vp8g3AEe",
        "A-o3T8Wz-Feb",
        "44mDLKmVyOUu",
        "cXqSyMyy-bm8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMwkByBnaE49"
      },
      "source": [
        "<p style=\"text-align:center;\"><img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBw8TEhIQEBISFRUVFhUVFhcVGBgZFRgYGRcYFh4WFRkYHSkgGiElGxUVITEhJikrMC8uGh8zODMuNygtLysBCgoKDg0OGxAQGy4lICUuLy0tNi8vLS0tLS0vLy8tLS8rLSs1LS0tLS0tLS0tLS8tLS0tLS0tLS0tLy0tLS0tLf/AABEIALkBEQMBIgACEQEDEQH/xAAcAAEAAgMBAQEAAAAAAAAAAAAABgcDBAUIAgH/xABQEAACAQICBAcJDAgDCAMAAAABAgMAEQQSBQYhMQcTIkFRYXEUMlRyc4GRstEXIyQzQlKCkpOhsdMWNFNiorPB8HSDoxUlNWPC0uPxQ8Ph/8QAGgEBAAMBAQEAAAAAAAAAAAAAAAMEBQIBBv/EADgRAAEDAgIHBwIEBgMAAAAAAAEAAgMEERIhBTFBUXGBkRNhobHB0fAUIiNScuEkMjM0RPEVQrL/2gAMAwEAAhEDEQA/ALxpSlESlKURKUpREpSlESlKURKUpREpSufpfS0GGjMuIkVFHTvJ6FA2seoUQm2ZXQqE638IWGweaKK0842ZFPJQ/wDMbp/dG3pte9QXW3hHxOIzRYXNBDuJv7846yO8HUvp22qB5auR0p1v6LPmrQMo+vsrU4L9YsVi8fO2JkLEwEqo2IgEibEXcO+G3edlyatiqR4FzbHSdcEg/wBSI/0q49I4tYY3lbcik9vQB1k2HnqKdv4mFo3KamkvDjcd91t0rmaH0zFiEzRnaN6neO0dHXXTqJzS02OtWGPa9oc03BSlKVyukpWDETpGpeRgqjaSdwqvdZNb2lvFh7pHuLbmf2dm88/RU8FM+Y2bq2nYFUq62Kmbd5z2Daf27zku3rHrekN44LPJuJ3qnt/Ac/RXa1fx4ngjl5yLN2jZ9+/ziqdqacHWkrO+HY7GGZO1RtA7Rt81aVVQsZBdmsZ8d/v7rGodKyS1VpMg7IDcdY5nVxtqVhUpSsZfSJSlKIlKUoiUpSiJSlKIlKUoiUpSiJSlKIlKUoiUpSiJSuVprTuGwicZiHyj5K73Y9CrvPN1DntVRa2a+YjF5oo7wwG4yqeW4/5jDm/dGzbtvU0ULpNWpV56lkIz17lN9beESDD5osLlmmGwm/vSH94jvj1DruRVRaW0niMTIZcRIztzX3KOhRuUdQrVy1+5a0YoGx6ljTVT5dercseWmWsmWlqlsq+JTHghNsf2xyD1T/SptwiaU2JhlO+zSf8ASv4/dVe8HmLWLHRyP3oWW/mjdrD6tdHHYtpZHlfvnYserqHUBYeavIIMU+M6gPHMeGviuqurwUnZN1uJ6ZX63twuvnCYqSJxJGxVhuI/vaOqrE1b1qjntHLZJdw+a3sPV6Oiq0pVyopWTizte9ZlHXy0rrszG0bD7Hv81elcrTWm4cMuaQ7T3qjefYOs1AsFrviIozEbSG1kc717fnf3v3VHsVi5JWMkjFmbaSf72DqrMi0a7H+Icu7b7c/FbtRppgjBiBxHfs9+WXkujpzTs2Ka7myg8lR3q+09f4bq5V6+b0vWwxrWDC0WC+bke6Rxc83JX1etnAYxopElXehDdtubzi489al6Xr02ORXIJabjWFeeHnV0WRDdWUMD1EXFZqiHB5pHPA0LHlRHZ4jXI9BzD0VL6+WmjMbyzcvvaeYTRNkG0eO0cjdKUpUamSlKURKUpREpSlESlKURKUpREpSlESlK4+ntYsNhFzTtyj3qLtduwdHWbCvQ0uNgvHODRdxsF1mIG07qgOtXCLFFeLB5ZX3GQ7Y18W3fn7us7qhutGuOJxl0vxcP7NTv8dvldm7ds56jOWtCGjtnJ0WPUaS/6xdfZZNIY2adzLM7O53sx29g5gOobK17Vly0y1eA3LKL7m5WK1LVly0y16mJYctMtZstflqJiWTR3xqef8DUgqP4H4xO2u5JIBvqxBqKo1Q+8cFkJrXknvsFYJJSa+b1KSo2x2zK+71+q9qx3pevF3ZbKsDX1WqGrIkleLgsWa9L1jvS9FzZd3VDSXE4qMk8luQ/YbbfNyT5quGqBvVyaq6S7ow0bk3YDI3Tddlz2ix89ZOk4tUg4H09l9HoSfJ0J4jyPp1K7VKUrKW+lKUoiUpSiJSlKIlKUoiUpSiJWKWRVBZiFUC5JNgAOck7q4+sOs2Gwg98OZyLrGtix6z80dZ67XqFav6z4nF6Sw/GNljBkyxLfKPensW+ces+YCpmQPc0v2AHw3KtLVRseI7/AHEgW3XNs/l1ua0cIqreLBWY7jKw5I8RT33adnUarbETvI7PIzM7G5ZtpPaTXo+lTRVTIx9rPH9lXnoZJjd0mW7DkPHzXmq1LV6VqpuFv9ai8ivryVbhq+1dhw25/sqFTo/sIy/FflbbxKgyRkkBQSTsAAuT2Ctr/ZeI/ZSfVb2VvapD4bhfKx+sKvqvamo7JwAF15RUbahhcSRY28LrzjNhJE2ujL4ykfiKwWr0oajGndSsHiBdUEUnMyAAX/fUbG+49dRMrwT9wt4qaXRTgLsdfiLeKpTLXxJXV03oiXCyGOcWI2hh3pX5ynorhSzX3VexAi4WWGODi06wulozR2JlIaCGSQKwBKIzAHrsK6ON0ZioxnmhkQE2u6Mov0XI6jUz4F/icR46+rXV4VD8DHlV9R6rNq3CbsrZEq+/R7DT9tc3AJ2d6qyKNmNlUsegAk/dWbuGb9m/1W9lSTgtPww+Tf8AFatyuqitMT8OG65pNGNmjxlxGvYvP/cM37N/qt7Kdwzfs3+q3sr0BSoP+SP5R1Vn/hWfnPRef+4Zv2b/AFW9lfMmFlUXaNgBvJUgekivQVRzhB/4fiexP5iV1HpAueG4dZtrUcuiGMjc/EcgTq3C6p1Jemst60s1Z8HG7ukabWdlUDrY2H41p4htWEYydS3psK6pHIwssgYoenK2U/fUr4NtJ5Jmw7HZKLr4w2/eL/VFd3XLQa9wqsYucOFK9JUDK1/Nyj4tVlg8U0bpKmxkYMO0G+2qjXiqhcOI9R6K++M0FS06xYHjlZ3qRyV+0rWwOKWWNJU711DDzi9jWzWEvqwb6kpSlESlKURKUpREpSo7rHrXh8KCpOeTmjU7fpH5I+/qrpjHPOFouVxJI2NuJ5sF3J50RS7sqqouWY2AHWTVd6zcIRN4sFsG4ysNv0FO7tPoG+orp3T+JxTZpW5IN1Rdir2DnPWdtcq1akFC1ucmZ3bP3WDVaVc/7Yshv28t3nwSV2YlmJZibksbknpJO+u/wfD/AHhhv8z+U1cC1SDg/H+8MP8A5n8pqtzD8J3A+RWfSn8eP9TfMK5ZjZWPUfwqkhrdpHwmT+H2VdmI7xvFP4V56Aqho9jXYrjd6rX0vK9mDCSNeo23LtfpbpHwmT7vZXO0jpCadg87s7AZQTa9gSbbOsn01r2patIRNGYA6BYrp5HCznEjiV09VB8NwvlU9YVe1UZqmPhmF8onrCrzrL0j/O3h6re0N/Sd+r0CpI6zY+KVyk8mxm2MxZbZjss9wPNVg6oa3Ji/epAEmAvYd645yl9uznH47bVJpacLK4G05m9Y761NH6ReGaPEKeVGwcc17b17CLjsNWpqdj25Cx6KjTVUscn3Elt9ufTdZXjrpq+uNwzx2HGLdom6GA70nobcfMeYV5+II2EWPODvHbXp9GBAI3EXFee9ecMItIYtBu4wv9cCT/rqnRv1tPFaOkYhYP5Kf8CvxGI8dfVrp8Kv6mvl09V65fAp8RiPHX1a6XC3+pL5ZPUkrkf3Q4rs/wBkf0lVfo/SU0D8ZC5RrEXFr2PNt7BXS/TDSPhMn8PsqPZqZq1S1hzIHQLBbJI0Wa4gdxKszg407ip8S8c8zuBEWANrXDIL7B0E+mpnrRiHjwmIkRirKhKkbwemq44IT8Ll8g3rx1Yeun6jivJtWVUNAnAAyyW/RucaW5Nzn6qpf0w0j4TJ/D7KwY3WXGyo0Us7sjWuptY2II5ukCuLmpmrV7Ng2DoFg9tKRm49SsmaprwW6N4zFNOw5MC3HjPdR92c+ioNmq7+D7RfEYOO4s8nvrfSAyj6oXZ03qCslwxEb8vdWdHQY5gTqGfspHKgYFWFwQQR0g7LVQ+l8IYMRLh23oxAJ513qfOpB89X5VXcLujMrxYtRsYcW/jC7KfOMw+iKpUMuGTDsPn8utHSlOJIsW1vlt9F1+DHSeaFsMx2xnMvisdoHY1/rCp1VC6o6b7lxUUjHkXyv4rbCTbfbY30avSCZHUOjBlYXBBuCOoiua2PDJiGo+e1d6MlxQhh1ty5bPDLks1KUqotBKUpREpSlEVca464YhXfDwo8Vrguws7c115lB22I2nZuqAtckk7SdpJ3k9Jq8dMaGgxKZJlvbvWGxl8U/wBN1VdrHqrPhSWPLi5nUbOxh8k/d11sUU0RGACx8+fp0uvnNJ00+IyE4m+XL166lHrUtX1av21aKxrr4tUh1AHw/D9sn8pq4NqkGoQ+H4f/ADP5T1DP/SdwPkVYpD/ER/qb5hXC63BB59lRj9AtHfsm+u3tqTSNYE9AJqvxwmHwT/W/8dYkDJnX7K/fnbhtHevqKuWmZh7e221xfdfYe5dv9AtHfsm+u3tqDa9aHhw08aQAhTGGNyTtLMN56lFd73TD4J/rf+OorrbrCMVIkzJxeVQuXNmvZi1wbD533Vfp46lsl5L2z1m/qVlVk1E+ItgAxZam228Fj1VHwzDeUT1hV415+1WxJbH4TmHHR7PpDfXoGq+kDd44epVzQ7C2I33+gXmnSHxsvjP6xrHhcK80kcEe1pGVB9I2uern81dV9BYyaeRYsPK13faEYL3x3sdg7SasrUDUbuU904gq2IIIUDasQOw2POxGwnm2gc5NiWdsbe9VYaV8kmYyupxGgACjcAB6K896/YoSaRxbLu4zJ50URn70NXVrfp9MFhnma2fvYl+dIRsHYN56ga86PISSzEkkkkneSdpJqrRt1u5K9pB9wGc1b3An8RifHX1a6PC6fgS+WT1JK53Aj8RifKL6lb/DEfgK+WT1JK5/yRxXZ/sz+kqn81M1Yc1M1al1g4VYHA6fhcvkG9eOrF11/UMV5JqrbgbPw2X/AA7fzI6sjXj9QxfkmrLqf645ei3aMfw3X1VAZq/c1Yc1M1al1hBq7eq+jO6sVDh/ks138ReU23m2AjtIr0IBbsqsuBzRWybGMN/vUfYLM567nIPompRwg6W7mwUrA2eT3pOm73uR2KGPmrMqnGSUMGzLqtyijEMJedufILjana18fj8XEW5EpzQbdnvfJ2eMgzfRNSfWzRfdOFmhA5RXMnjryl9JFuwmqE0NpFsPPFiE3xuGt0gb184uPPXo7DTK6LIhBVlDKRuIIuCPMa8qWdm8Ob8IXtHL20bmv7+hzXmnNUu1T1onw1shzJflox5J616Dbn9N60uEPRfc2OlAFkk99Tscm48zhxbotXD0fNZrdP41psc2RuYuCsSVkkLjhNnN2/N42L0FobTMGJTPE20d8p2Mp/eH9d1dWqCwOOlhcSwuUYbiPwI3EdRq0NVdcYsTaOW0c24fNfxSdx/dPmvzZ9TROj+5mY8R78VrUWkmzfY/J3geHf3KW0pSqK1EpSlESsciAgqwBBFiDtBB5iKyUoir/WbUS95cHs5zETs+gTu8U+Y81QGSJlJVgVINiCLEHoIO6r+rg6w6t4fFC7DLIByZFG3sYfKHV6CK0aevLftkzG/aPfz4rGrdFNfd8OR3bDw3eSpy1SDUMfD8P/mfynrU01oKfCvlkXYe9cbVbsPT1HbW5qJ+v4f/ADP5T1pyuDoXFpuMJ8isSnY5lUxrxY4m+YVtzd63in8KoICr/kW4I6QRVW4ng6xxFkmw46SS9/NyKzKCZkQdjNtXqtzStNLOY+zF7XvzsoVi8WqbN7dHR21ypZSxuxvU79yjH/tsN9aT/sqLazaAmwUqwzNGzMgkBjJIsWZbcoDbdDVr6lkhsCqP0T4W3I5rJqafh2E8tH6wr0TXnTUw/D8H5aP1hXouqFb/ADDgtbRw/DPH0CVG9ZNcsFggeNkDSc0SEGQnrHyB1tbz1RmmNYMa0sytisQVzuMplky2DEWtmtXEBrxtMNpXr638o6ru60ayT46bjpjYC4jjHexr0DpJsLtz9QAA496w5qZquCwFgs913G51q5+A8+8Ynyi+pW/wyH4Cnl09SSubwFn3jFeVT1K6HDQfgCeXT1JKo/5HNaf+LyVMZqZqwZqZq0MSx8KsTgXPw2X/AA7/AMyKrL16/UMX5JqrDgUPw6X/AA7/AM2GrO18/wCH4vyTVnzn8botilH4HX1XnfNX0gJIVQSSQABvJOwAVr5qmPBXojujHo7C6QDjm6Mw2IO3MQ30DV90mEErJjiL3Bo2q6NXNGDDYaHDi3IQBiOdztY+dixrg69apT49oss6RpGG5JUklmIuTY8wAt2mpRjsUkUbzSNlSNS7HabKouTYbTsG4VG/dK0P4SfsZ/y6y2GTFibr4LdkEeHA7VxUS9yKfwuP7Nv+6rC1W0bJhsNHh5ZBIY7gMARyb3AIPQDbsArle6Vofwk/Yz/l1lwev+ipZEhjxN3dgigxzLdmNgLsgAudm013I6V4+4eCjiZBGfsI6rjcMOieMwqYlRyoGs3k3sp7bME7BeqbD22ivTmkcGk0UkMgusiMjdjAj+teZdIYV4ZZIJO+jdkbtUkXHUbXFWKST7cO5U6+H7g/euzFLmAYc9fV65ui59hXo2j+v99db2atVrri6+fkjwuLVPdVdfGS0WMJddwk2ll8bnYde/t5rKgmR1DowZWFwym4I6QRXnpASQqgkncBtJ7BU41Nw+l4WHFwOYieUspyL4y5tqnrF784NZ9VTM/mabHoCtigrpScDwXDeBcjjbZ48dlp0rDxj/M/iFKy1uXWalKUXqUpSiLXxWGjkQxyKGU7wwuP766hUmg4MBiocWZkSDM4IdgGUtGwAX54ue0de010dcddsPgQUFpJyNkYPe3+VIfkjq3ns2im8bj8ZpHEAu3GSNmyrdURVALELmIVQApO0820k1bphIAc7NIz+eqz6wxYm3bdwII7rd/orw/TTRnhcXpPsp+mmjPC4vSfZVGT6ClMkiRNFIsYDGUSwiKxOUFpOMKKSb2UtmPRWOLV7GNcLGLh2jAMkQLunfJEGcGUjoTNXX08f5k+ql1YPNXv+mmjPC4vSfZVVcKmlMPiMVG8EiyKIVUld2YSSG3oI9NRHuOW8Qy7Ztse0cr3xounZy0YbbbuivrSOAkgOWUx3uwISWKQgrsIcRO2U7dxtz9BqSOFrHXBzUE1Q+RhBbl8K3tVsSkeMwskjBUWVGZjuADC5NXi2vGihvxkPp//ACqMn1bxqMqNGuZpFhsssT2lbdG5RyEJ6GtXLl0RiijziImNIknYgqbROzKshAN7Eq19myxJsNteTNY+xuuqZ0kV2hvesOkJQZZWBuC7kHpBYm9a+augmruMLOvFqpQxqxklhjXNIodEDyOFZipBygk9Vfg0LIMO2JdokG3i1aWANIEZlkKK0gc5SluSrFiRYHfXWMb152TidS0M1M1dGXVzGrI8LQsHRoUZSyd9O2WOxzWIY7LgkDnIrDLofELGJXESqc5GaeBXbi3aNskZkztZ0YbFN7bL0xjevOxduVkcD+sOCw0OJXEzxxFpFKhja4y2uK3OFjWXBYnBpHhsRHI4nRiqm5sEkF/SR6aq2DQeLeR4VivIkwgZcyC0p4yyXLW/+GXaDbk79ovk/wBgYoWusdihkEnHwcTkDiMnjuM4vY7Kts17sBziocLcWK6nxSdngw9y0c1fuetjDaLnkkkhRVzxh2fNJGqKENmJkdglh03r7OhcVlMgQMglSEvHJHInGOAyrnjYrtzAZr2ubE32VPjCqiJxzAUm4LNOYbCYuSXFScWhhdAcrNyjJEwFlBO5W9FT3W/XvRc2CxMMOJDO8bKq5JRcnmuUsKpU4CbNOuTbhw7TC68gI4jY79tmZRsvv6K3JNXMasrwNCwkSSGJlLILPMSI1vmsc1jygbdJFROYwuxEqxG+RrMIbvXPz1b/AAU6R0dhcIzzYrDpNM5ZlZ1DKq3VVbb4zfTqrU0HiGYqvEtZTIzLiMO0aICAWkkWQoguQOURe+yvtdXMYWkXi0Ux8XmLzQonvoJjKu8gVwwU2Kk3tXUlnixK4ia6N2INurP4VdccNJhBhsJPHKZXHGcWwbKicqxI3XbJ2gNVQZq3xq5jrxqMO95Znw6Dk7ZUNmQ7eSRY7WsLBjewJGFdD4opPIImyQMqStdbKzMUA38rlC2y9ri+8XRhrBYFJsbzicLLWz19JMVIZSQQQQRvBG0Eeeuo2q2MGQEREyBmQJPA5ZVV2LAJIeSBFJt3XW2/ZX6NXMQI+OKXXi+NtnTPxd7cbxQbjMn72W1tt7VIHA7QoSwjYeivDQ+vej5IIZJcVBHIyKXRnAKvblLY9d6q/hTGGkxYxOEljlEyDjMjA2dLLc9F1yfVauImgcTneIRcuNokYZk2NIQqC+axuWA6ue1YMdg3iID8Vc3+LkiltbeG4p2y+e1Rxwsa64cpZqh72WLOeaxaNjAkQyNlTMA5AuQpNmIHOQLmr10bwf6PSxIeXnBZtnmCWBHbeqGzVe/BjpnujAorG7w+9N2Acg/VsL9Kmvalz2tBaSAuaJrHvONoJ2KS4LAQxC0MUcY/cUL+A21t0pWctgZCwSlKURKUpRF8OwAJJsBtJO6qs134TgM2H0ewJ2hp94HVD0+Pu6L3uLF0zoiDFRmHEBmjJ2qryID1NxbAsOo7K4PuaaG8FP20/wCZUsZYM3KGUSEWYQFQckxYlmJLEkkkkkk7SSTtJ663tX9K9zYiPEZS2TPyQwUnMjJsJUgWzX2g7qu73NNDeCn7af8AMp7mehvBT9tP+ZVk1LDrBVJtFIDcEKoZ9YIJDKssM7xy8Ux98jEyvFnCsrLCEtlkYFSnPe9MPrDhxxObDOe5pHkw4EtgAziQJNeMlwGBN1yk3t1i3vcz0N4Kftp/zKe5nobwU/bT/mVz20e4/Oak+nlve46fsqcOnom4iSSKVpoSSGWRVja+IknOaPiiRtlYbG5h2Vh1n04uLcOFlU3ckSPG4GYggJxcSEAbe+zHdt33un3NNDeCn7af8yuPp7gtwRXNhI8rW2o0khDdjF7g9pt2V7HLGXDZx1eq4limDD/27ha58rqstP61K4xBgieJ8VIskrNKHPJJIWPLGmUXbebnrrBh9cpI0CxJldYcNCGLZlIhklc5ky8pXWUoVvuvtN60sZhYS7ZV5O4cpzsHPv599Ye4ovm/e3tqYwbFAyrNr2N+AXcfXWN5XkfDMF45Z4ljkW6MIEw7IxkidXRljX5IK22GtfHa3LLhXwxikTMcQRxckQi9+leUAo0DNZSwFldbgc1cvuKL5v3t7adxRfN+9vbXP067+sPf0CkE/CA7s5eAEHEwYiPl8tFjmExgLZeUpYEjYMpZjtvatHF61rJhO5Sky246xWSLiyZJ5JwWVoC+wyAcl1vl5r1ze4ovm/e3tp3FF83729te/Tp9Ye/oF2U1whSfuiPDMGfFLi5w0wZWdRMMkXvYyLeeRtuY7QNw248JrfZoHZJEMcDQnuVooYzmk4wuIeJaMZtgdSpViA2wgW5XcUXzfvb210NF6tS4j4jDyybbXXOVB623DzmuTABr816Ktx1A9AvzResyQ4ufFLBkWZZUEcTKvFiQgjIWjZdlvmW6hurbw2urRk8XGzK2I46QSurcYhiSNon4uNF3oGDBRlIXYSLmQaO4IsXJYyiOEc+Zyz+YISP4hUp0bwOaPWxneWU84UmND5gS38VRuMY234X/ANKePtTsI42/2qgbTnLx75P1tJktm+L4yZJr7uVbJbmve/VXeGvUjuzPAG+GR4tDm5aokzzjDF8vKUNI+U/JzNssQBbScF+hBuwh+2n/ADK+/cz0N4Kftp/zK47SM67/ADmpOzlGoj5yVJYbTsr8amMHGxzRpG/F8XFIOLk4xWQrGVuGLb1NweoW6MOtSpdVw8bIBhERJskoEeGMjWfOlmZzKxzqFy81qtz3M9DeCn7af8ynuZ6G8FP20/5lddrFuPzmuOxn/MPnJVNDrpNHlEQcLxsskmZwzSCSUSWLZRZrB1LW2iRtgvavqDXEKGj7mQxuZzKCzcYxmJvlYbFsoiAup2pfn2Wv7mehvBT9tP8AmU9zPQ3gp+2n/Mp2sW4/Oa8EM/5h85KmsLrBkfDPxd+Iglhtmtm4zujl7tlu6N23vd+3Ztx6zQZJC2FJxDwcQZRIAoAgaASKmS4JQgMuaxy3Fja1te5pobwU/bT/AJlPc00N4Kftp/zK9M8Z2H5zXgp5htHzkqq/S5A/HLA3GPLhpJryAo3c5DWiXJdMxUEklrbhXL1h0yuJdXAlBAIPGPG283AXi4o7c++9XT7mmhvBT9tP+ZT3NNDeCn7af8yvBPGDcA/OaOp5nCxIt87lQGeptwS6b4jHCJjZMSOLPRnG1D6cyjx6sn3M9DeCn7af8yvqLg40OrK64YhlIZSJp7gg3BHvnSK6fUMc0jNcx0j2ODgRkpbSlKpLRSlKURKUpREpSlESlKURKUpREqF8KGnu5sGY0NpMReNekLblt6CB1FhU0rz5rppSbSONkOGSSVI/eoxGrPyQTd7LfvmzG/Rl6KmgZidc6gq9S8tZYazkoyWr8JqW6M4NdKy7WjSEdMrgfwpmb0gVLNG8EEQ24nEu/wC7EoQdmZs1/QKvOqGDasxlHKdluOSqXNWSKN271WbsBNW1prg2hjHGYSMPbej8pu1Sf/fRfdUZK22WtbZbdbqtVqna2ZuIO9ws+rlfTPwuZwOw8PgUZh0PMd4C9p/oL1vQ6CUd+5PZsH9a69Kttp2DvWc+rldttwWrDgIV3IO07T99Wpwcn4Kw6JW9VarWrG4OG94kH79/SB7KraQAEBtvCv6GcTVZnYfT2UupSlfPr61KUpREpSlESlKURKUpREpSlESlKURKUpREpSlESlKURKUpREpSlESlKURfDoCCCAQdhB3EdBr5hhRAFRVVRuCgADsArLSiJSlKIlcDT+rUOJBa2STmcD7mHP27/wAK79K7ZI6N2JpsVHLEyVpY8XBVM6U0XNh3ySrboI2gjpB5/wAa0auvGYSOVDHKoZTzH8QeY9YqvdYtUJIbyQ3kj3kfKXtA5usefprbpa9sn2vyd4H2Xy1doh8P3xfc3xHuO/8AcqLVP+DVuROOgofSD7KgFTrgxP6yPJf/AGVLpAfw7uXmFX0Of4xnff8A8lTqlKV84vtUpSlESlKURKUpREpSlESlKURKUpREpSlESlKURKUpREpSlESlKURKUpREpSlESlKURKUpREpSlEUU1g1RjmvJDaOTeR8h+23ens9HPWlqDhZIpcRFKpVgqEg9rbR0jbvFTitc/GDxW9ZatCqeYjE7MW6WIKoOoIhO2oZkQc7ajcW656+u9bFKUqqr6UpSiJSlKIlKUoiUpSiJSlKIlKUoiUpSiJSlKIv/2Q==\" alt=\"EPITA Lab 1\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKtcL0CYn2n_"
      },
      "source": [
        "# Part 1. Keywords Extraction (14 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXcmrzodG1pt"
      },
      "source": [
        "## What is Keyword Extraction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZWKLXCYG9NJ"
      },
      "source": [
        "Keyword extraction is defined as the task that automatically identifies a set of the terms that best describe the subject of document. This is an important method in information retrieval (IR) systems: keywords simplify and speed up the search. Keyword extraction can be used to reduce the dimensionality of text for further text analysis (text classification ot topic modeling). S.Art et al., for example, extracted keywords to measure patent similarity. Using keyword extraction, you can automatically index data, summarize a text, or generate tag clouds with the most representative keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O3FI910HETW"
      },
      "source": [
        "## How to extract the keywords?\n",
        "All keyword extraction algorithms include the following steps:\n",
        "\n",
        "* Candidate generation. Detection of possible candidate keywords from the text.\n",
        "* Property calculation. Computation of properties and statistics required for ranking.\n",
        "* Ranking. Computation of a score for each candidate keyword and sorting in descending order of all candidates. The top n candidates are finally selected as the n keywords representing the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD64EYyAHE5k"
      },
      "source": [
        "# all the imports \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3WHRR0C9Il"
      },
      "source": [
        "## Goal.\n",
        "\n",
        "In the following, given a paper, we will extract the keywords associated to this paper. Each individual can have their own qualitative assessment of what is \"key\" word. However, we will try as much as possible to objectify the approach and quantify to what extent a keyword is indeed key to the paper in question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUQe5Df4Bpr"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT3cUUST9QX6"
      },
      "source": [
        "%%capture\n",
        "! git clone https://github.com/MastafaF/ExtractKeywords.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJTpCLQ9XJ5",
        "outputId": "f7fde563-4c96-4951-df52-d2e7b3d6e5a4"
      },
      "source": [
        "import os \n",
        "\n",
        "os.listdir(\"./ExtractKeywords\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README.md', 'LICENSE', '.git', 'data.tar.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVjwTfE9dyP",
        "outputId": "29fd68e6-1b82-4689-e2e7-df77f1e3fbd5"
      },
      "source": [
        "# Extract data file \n",
        "\n",
        "! cd ExtractKeywords && tar -zxvf data.tar.gz data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/papers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "CdRVtKw44C9p",
        "outputId": "48266720-0e4c-45ad-92c6-988dc00b1178"
      },
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('./ExtractKeywords/data/papers.csv')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41d458ac-1756-40f8-879c-a9dd18caa08e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41d458ac-1756-40f8-879c-a9dd18caa08e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41d458ac-1756-40f8-879c-a9dd18caa08e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41d458ac-1756-40f8-879c-a9dd18caa08e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IElpuJ76o0"
      },
      "source": [
        "## Preprocessing data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRAj0JXjAWBo",
        "outputId": "0446cc78-7251-4d6e-8284-265b8dde8f11"
      },
      "source": [
        "# For the Lemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHC9ShM-IX7E"
      },
      "source": [
        "### Question 1.1: Preprocessing data in a meaningful way [code] (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWpI1OJk78Gc"
      },
      "source": [
        "import re\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "\n",
        "# Me \n",
        "from gensim.parsing import preprocess_string, strip_short, strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
        "\n",
        "# Update stop words accordingly\n",
        "#my_stop_words = STOPWORDS.union(set(['mystopword1', 'mystopword2']))\n",
        "my_stop_words = STOPWORDS.union(set(['\\n', '~\\n\\n']))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "\n",
        "stop_words = STOPWORDS.union(set(new_words))\n",
        "\n",
        "def pre_process(text):\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  CUSTOM_FILTERS = [\n",
        "    lambda s: s.lower(),\n",
        "    lambda s: re.sub(r'\\s+\\w{1}\\s+', ' ', s),\n",
        "    strip_tags,\n",
        "    strip_numeric,\n",
        "    strip_punctuation, \n",
        "    strip_multiple_whitespaces,\n",
        "    strip_short,\n",
        "    #remove_stopwords, # Removes all English generic stopwords\n",
        "  ]\n",
        "\n",
        "  text = preprocess_string(text, CUSTOM_FILTERS)\n",
        "\n",
        "  lemmatized_text = []\n",
        "  wnl = WordNetLemmatizer()\n",
        "  for word in text:\n",
        "    lemmatized_text.append(wnl.lemmatize(word))\n",
        "\n",
        "  text_wo_stopwords = [word for word in lemmatized_text if not word in stop_words]\n",
        "  final_text = ' '.join(text_wo_stopwords)\n",
        "  \n",
        "  return final_text\n",
        "\n",
        "  # ------------------"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZFsz9JX_9I0",
        "outputId": "43d6de88-13dc-4b45-a781-64c6fd882fad"
      },
      "source": [
        "%%time\n",
        "df['preproc_text'] = df['paper_text'].apply(pre_process)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 14s, sys: 531 ms, total: 2min 15s\n",
            "Wall time: 2min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_3ADgBBsAHhg",
        "outputId": "04b2d52a-03ed-4f4d-b64e-6d16a62f0b96"
      },
      "source": [
        "# Visualizing data \n",
        "HTML(pd.DataFrame(df.loc[0, [\"preproc_text\"]]).to_html())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>preproc_text</th>\n",
              "      <td>self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed application robot eyesight proposed database associate input output half discussion algorithm self organization proposed aspect hardware produce new style neural network half applicability handwritten letter recognition autonomous mobile robot demonstrated introduction let mapping given finite infinite set finite infinite set learning machine observes set pair sampled randomly mean cartesian product computes estimate small estimation error measure usually faster decrease estimation error increase number better learning machine expression performance incomplete lack consideration candidate assumed preliminarily good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating parameter indefinite structure equivalent define implicitly set candidate subset mapping computes value parameter based observed type parameter type learning machine defined approach number increase alternative case estimation error remains eternally problem designing learning machine return proper structure sense hand assumed structure demanded compact possible achieve fast learning word number parameter small parameter uniquely determined observed demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge given unknown case comparatively easy proper compact structure alternative case difficult possible solution compactness assume almighty structure cover combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed discussion flourishing efficient method proposed recently hardware unit computing coefficient parallel speed sold anza mark iii odyssey neural network exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose located near case estimation error negligible distant estimation error negligible research report following situation appears complex estimation error converges value number increase decrease hardly dimension heighten property considerable defect neural network recursi type recursive type founded methodology learning follows initial stage set instead notation candidate equal set mapping observing reduced observing second reduced candidate set gradually small observation proceeds observing write likelihood estimation selected contrarily parameter type recursive type guarantee surely approach number increase recursive type observes rewrite value correlated type ha architecture composed rule rewriting free memory space architecture form naturally kind database build management data self organizing way database differs ordinary following sense doe record observed computes estimation database associative database subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari dissimilari mean mapping real necessarily defined single formula definable example collection rule written form dissimilarity defines structure locally knowledge imperfect flect heuristic way contrarily neural network possible accelerate speed learning establishing especially easily simple process analogically information like human application paper recursive type strongly effectiveness denote sequence observed simplest construction associative database observing follows algorithm initial stage let set let equal min furthermore add produce version improved economize memory follows algorithm initial stage let composed arbitrary element let lex equal min furthermore let add produce construction approach increase computation time grows proportionally size second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence algorithm constructing associative database follows algorithm step initialization let root root variable assigned respective node memorize data furthermore let step increase reset pointer root repeat following arrives terminal node leaf notation nand let mean descendant node let step display yin related information yin step establish new descendant node secondly let yin yin yin finally step loop step stopped time continued suppose gate element artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letter elastic matching method recognizing hand written discrete english letter global training fuzzy logic search method recognizing chinese character written square style published self organization associative database realizes recognition handwritten continuous english letter nov dwlo source document loo windowing number nualber sampl experiment scanner document letter recognizer parallelogram window cover maximal letter process sequence letter shifting window recognizer scan word slant direction place window left vicinity black point detected window catch letter succeeding letter recognition head letter performed end position boundary line letter known starting scanning boundary repeating operation recognizer accomplishes recursively task major problem come identifying head letter window considering define following regard window define accordingly denote black point left area boundary window project window measure euclidean distance black point closest let summation black point divided number regard couple reading position boundary define accordingly operator teach recognizer interaction relation window reading boundary algorithm precisely recalled reading incorrect operator teach correct reading console boundary position incorrect teach correct position mouse partially document experiment change number node recognition rate defined relative frequency correct answer past trial speciiications window height dot width dot slant angular deg example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case doe attain distinguishable excessive lluctuation writing consistency relation assured like number node increase endlessly clever stop learning recognition rate attains upper limit improve recognition rate consider spelling word future subject obstacle avoiding movement camera type autonomous mobile robot reported flourishingly author belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator motion robot learning natural length width height robot weight visual angle camera deg robot ha following factor motion turn le deg advance le control speed le experiment wa passageway wid inside building author laboratory exist experimental intention arrange box smoking stand gas cylinder stool handcart passage way random let robot camera recall similar trace route preliminarily recorded purpose define following let camera face deg downward process low pas filter scanning vertically filtered search point luminance change excessively bstitu point white point black obstacle exists robot white area free area robot regard binary dot processed define accordingly let number black point exclusive regard obtained drawing route define accordingly robot superimposes current camera route recalled inquires operator instruction operator judge subjectively suggested route appropriate negative answer draw desirable route mouse teach new robot opera tion defines implicitly sequence reflecting cost criterion operator iibube roan stationary uni configuration autonomous mobile robot north rmbi unit robot roan experimental environment wall camera preprocessing preprocessing course suggest ion search processing obstacle avoiding movement processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency attains time notice rest doe mean directly percentage collision practice prevent collision adopting supplementary measure time number node wa level tree distributed proposed method reflects delicately character operator example robot trained operator slowly space obstacle trained operator brush quickly obstacle fact hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera position data respectively memory capacity finite actual compu ters compress camera slight loss information compression admittable long precision position identification acceptable area major problem come suitable compression method experimental environment jut passageway interval section adjacent jut ha door robot identifies roughly surrounding landscape section place temporarily triangular surveying technique exact measure necessary realize task define following turn camera panorama deg scanning horizontally center line substitute point luminance excessively change black point white regard binary dot line processed define accordingly project black point measure euclidean distance black point closest let summation similarly calculate exchanging role denoting number respectively nand define regard positive integer labeled section define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway area learns relation landscape position data position identification area achieved crossing plural database task automatic excepting periodic reset counter kind learning teacher define identification rate relative frequency correct recall position data past trial typical example converged time time number level wa level oftree distributed identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera loosened possibility depends improvement hardware future example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving time interval frame sec actual motion robot conclusion method self organizing associative database wa proposed application robot eyesight machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced subclass structure subject imposed widen class probable solution abolish addressing rule depending directly value instead establish rule depending distribution function value investigation reference hopfield tank computing neural circuit model science rumelhart learning representation propagating error nature hull hypothesis generation computational model visual word recognition ieee expert fall kurtzberg feature analysis symbol recognition elastic matching ibm develop wang suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami brook self calibration motion stereo vision mobile robot int symp robotics research goto stentz cmu mobile robot navigation ieee int conf robotics automation madarasz design autonomous vehicle disabled ieee jour robotics automation triendl kriegman stereo vision navigation building ieee int conf robotics automation turk video road following autonomous land vehicle ieee int conf robotics automation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLS9mbe8B4UY"
      },
      "source": [
        "## 0. Raw counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WY9c6iwI0Lv"
      },
      "source": [
        "### Question 1.2: Build a top N words based on occurence [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izLhCAH5B-jy"
      },
      "source": [
        "\"\"\"\n",
        "Idea: \n",
        "\n",
        "0. Split with spacy OR nltk \n",
        "\n",
        "1. Counter \n",
        "\n",
        "2. Surface top 10 \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_counter(txt_preproc, N=10): \n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    tokens = nltk.word_tokenize(txt_preproc)\n",
        "    count_tokens = nltk.FreqDist(tokens)\n",
        "\n",
        "    count_tokens_list = list(dict(count_tokens).items())\n",
        "    count_tokens_list.sort(key=lambda ele: ele[1], reverse=True)\n",
        "\n",
        "    return count_tokens_list[:10]\n",
        "    # ------------------\n",
        "\n",
        "df[\"Top N\"] = df[\"preproc_text\"].apply(get_counter)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Xt7vPHWTaDCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "710f90a4-3fc8-47b1-bf0c-5db9af07a6cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "7236   994  1994                Single Transistor Learning Synapses   \n",
              "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
              "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
              "7239   998  1994  Learning direction in global motion: two class...   \n",
              "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
              "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
              "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
              "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
              "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
              "\n",
              "              abstract                                         paper_text  \\\n",
              "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...   \n",
              "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...   \n",
              "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...   \n",
              "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...   \n",
              "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...   \n",
              "...                ...                                                ...   \n",
              "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...   \n",
              "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...   \n",
              "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....   \n",
              "7239  Abstract Missing  Learning direction in global motion: two\\nclas...   \n",
              "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "0     self organization associative database applica...   \n",
              "1     mean field theory layer visual cortex applicat...   \n",
              "2     storing covariance associative long term poten...   \n",
              "3     bayesian query construction neural network mod...   \n",
              "4     neural network ensemble cross validation activ...   \n",
              "...                                                 ...   \n",
              "7236  single transistor learning synapsis paul hasle...   \n",
              "7237  bias variance combination square estimator ron...   \n",
              "7238  real time clustering cmos neural engine serran...   \n",
              "7239  learning direction global motion class psychop...   \n",
              "7240  correlation interpolation network real time ex...   \n",
              "\n",
              "                                                  Top N  \n",
              "0     [(robot, 23), (database, 19), (let, 18), (lear...  \n",
              "1     [(cell, 51), (network, 42), (cortical, 32), (s...  \n",
              "2     [(input, 58), (weak, 42), (synaptic, 36), (ass...  \n",
              "3     [(loss, 42), (query, 25), (model, 23), (functi...  \n",
              "4     [(ensemble, 56), (network, 52), (error, 51), (...  \n",
              "...                                                 ...  \n",
              "7236  [(gate, 44), (weight, 42), (synapse, 42), (cur...  \n",
              "7237  [(estimator, 37), (variance, 29), (bias, 25), ...  \n",
              "7238  [(current, 35), (chip, 21), (input, 19), (patt...  \n",
              "7239  [(learning, 69), (motion, 42), (direction, 32)...  \n",
              "7240  [(model, 54), (view, 47), (expression, 46), (f...  \n",
              "\n",
              "[7241 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-832ad6c4-1d2f-4f77-8976-34a3ee954aa3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "      <td>self organization associative database applica...</td>\n",
              "      <td>[(robot, 23), (database, 19), (let, 18), (lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "      <td>mean field theory layer visual cortex applicat...</td>\n",
              "      <td>[(cell, 51), (network, 42), (cortical, 32), (s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "      <td>storing covariance associative long term poten...</td>\n",
              "      <td>[(input, 58), (weak, 42), (synaptic, 36), (ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "      <td>bayesian query construction neural network mod...</td>\n",
              "      <td>[(loss, 42), (query, 25), (model, 23), (functi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "      <td>neural network ensemble cross validation activ...</td>\n",
              "      <td>[(ensemble, 56), (network, 52), (error, 51), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>994</td>\n",
              "      <td>1994</td>\n",
              "      <td>Single Transistor Learning Synapses</td>\n",
              "      <td>NaN</td>\n",
              "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "      <td>single transistor learning synapsis paul hasle...</td>\n",
              "      <td>[(gate, 44), (weight, 42), (synapse, 42), (cur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>996</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "      <td>bias variance combination square estimator ron...</td>\n",
              "      <td>[(estimator, 37), (variance, 29), (bias, 25), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>997</td>\n",
              "      <td>1994</td>\n",
              "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
              "      <td>NaN</td>\n",
              "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "      <td>real time clustering cmos neural engine serran...</td>\n",
              "      <td>[(current, 35), (chip, 21), (input, 19), (patt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>998</td>\n",
              "      <td>1994</td>\n",
              "      <td>Learning direction in global motion: two class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "      <td>learning direction global motion class psychop...</td>\n",
              "      <td>[(learning, 69), (motion, 42), (direction, 32)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>999</td>\n",
              "      <td>1994</td>\n",
              "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "      <td>correlation interpolation network real time ex...</td>\n",
              "      <td>[(model, 54), (view, 47), (expression, 46), (f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows  9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-832ad6c4-1d2f-4f77-8976-34a3ee954aa3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-832ad6c4-1d2f-4f77-8976-34a3ee954aa3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-832ad6c4-1d2f-4f77-8976-34a3ee954aa3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-J0RHMMFW6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e2ee03-d5f8-4999-a5e6-c5b0eb95eebd"
      },
      "source": [
        "df.loc[2, \"Top N\"]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('input', 58),\n",
              " ('weak', 42),\n",
              " ('synaptic', 36),\n",
              " ('associative', 35),\n",
              " ('ltp', 30),\n",
              " ('strong', 26),\n",
              " ('phase', 26),\n",
              " ('long', 24),\n",
              " ('stimulus', 23),\n",
              " ('hippocampus', 22)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7YbNCyJaqk"
      },
      "source": [
        "### Question 1.3: What are some of the limits of raw counts? How could we improve the approach through preprocessing? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eewWd_TjJlt-"
      },
      "source": [
        "Counting the number of times each word appears in a document is a very simple approach to text processing, but it has several limitations. First, it does not account for the order of the words in the document, so two documents with the same words in different orders will be considered to be identical. Second, it does not account for different forms of the same word, so \"fish\" and \"fishing\" will be considered to be different words. Third, raw counts could be very low for rare words, and this could also skew the similarity results. Finally, it does not account for the context of the words in the document, so two documents with the same words but different meanings will be considered to be identical.\n",
        "\n",
        "One way to improve the approach is to use a technique called \"stemming\" to reduce each word to its base form before counting. This will account for different forms of the same word. Further, we could use a weighting scheme such as tf-idf. This will palliate rare words getting low scores and skewing the results. Another way to improve the approach is to use a technique called \"stopword removal\" to remove common words such as \"a\", \"the\", and \"of\" before counting. This will account for the context of the words in the document.\n",
        "\n",
        "We've implemented in question 1.1 both the removal of stop words and the lemmatization which is the process of grouping together the different forms of a word so they can be analyzed as a single item."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wao4NivXGPIM"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRcD4MeHFnt"
      },
      "source": [
        "### Introduction.\n",
        "\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZe8obbHm-Q"
      },
      "source": [
        "### CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pd54cOkGV_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d333772-d718-4883-b69d-3445aa0706a1"
      },
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "\n",
        "\n",
        "word_count_vector=cv.fit_transform(df[\"preproc_text\"])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 14s, sys: 4.74 s, total: 2min 19s\n",
            "Wall time: 2min 19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fngZVCVGcG_",
        "outputId": "873783c7-7eca-4766-ad26-c78bccab52ab"
      },
      "source": [
        "word_count_vector"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 5316905 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxV_evcXHuKp"
      },
      "source": [
        "### TfidfTransformer to Compute Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfwFzHCHzOY",
        "outputId": "2ffdc4ee-1f22-4ed3-b54b-23f51dce3a40"
      },
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,\n",
        "                                   use_idf=True)\n",
        "\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 28.9 ms, sys: 988 s, total: 29.9 ms\n",
            "Wall time: 39.1 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd9hXGWJHzLy",
        "outputId": "5599033f-ebd2-46ba-8eb8-51a387322089"
      },
      "source": [
        "tfidf_transformer"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1wbWizrJ5CE"
      },
      "source": [
        "### Question 1.4: How can you find an optimal max_df? Why are we using a sparse matrix instead of a regular matrix? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EekhsGbKYoyv"
      },
      "source": [
        "The max_df parameter in a CountVectorizer indicates the maximum frequency of a word allowed in order to be included in the vocabulary. A higher max_df results in a smaller vocabulary.\n",
        "\n",
        "One way to find an optimal max_df is to iterate over different values and compare the results. Another way is to use a grid search to test a range of values and find the best one.\n",
        "\n",
        "There are a few reasons for using a sparse matrix with a CountVectorizer. First, a regular matrix requires a lot of memory, and a sparse matrix uses much less memory (because 0 values do not take up memory). Second, a sparse matrix can be created much faster than a regular matrix. Finally, a sparse matrix can be used with a much larger dataset than a regular matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_9miFxKaU1",
        "outputId": "81d6c088-aec7-4a76-8dfb-44987cbb0b41"
      },
      "source": [
        "cv.transform([\" change number node recognition rate defined relative frequency\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "K2Vdn9NTMIxI",
        "outputId": "f24066ca-11ae-44df-8b06-9b3e2eb39990"
      },
      "source": [
        "# Visualizing data \n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "plt.spy(csr_matrix(cv.transform([\"change number node recognition rate defined relative frequency\"])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7fa9d614e100>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAAlCAYAAABBEVJBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAGSklEQVR4nO3cX4xcZRnH8e+v3XZLwdAtEgSKaRuJpDfQhpAa1BgwgEisF1w0aiwCIQEvQBNICVfeGCVK1MRIDEQFFNBCAJsYUpFbC0WhIv03ULCFIqVA+dMIFB4v3mc7J5tud7cz3Zmd9/dJTua873l35rxPn3l65syZo4jAzMwG36xe74CZmU0PF3wzs0q44JuZVcIF38ysEi74ZmaVcME3M6tEXxZ8SZdI2iapJWltr/fnWJB0hqTHJT0n6d+Srs/+hZI2SNqRjyPZL0m/yJhslrSi8VxrcvwOSWt6NadOSJot6Z+S1md7iaSNOd/7Jc3N/uFst3L74sZz3Jz92yRd3JuZdE7SAknrJG2VtEXS52rMC0nfy/fGs5LulTSv5rzoiojoqwWYDTwPLAXmAs8Ay3q9X8dgnqcCK3L9E8B2YBlwK7A2+9cCP871S4G/AAJWAhuzfyHwQj6O5PpIr+d3FPH4PvAHYH22/wiszvXbgWtz/Trg9lxfDdyf68syV4aBJZlDs3s9r6OMxe+Aq3N9LrCgtrwATgd2Asc18uGKmvOiG0s/HuGfB7Qi4oWI+AC4D1jV433quojYExH/yPV3gC2UJF9FecOTj1/P9VXAXVH8HVgg6VTgYmBDRLwREW8CG4BLpnEqHZO0CPgqcEe2BVwArMshY+MwGp91wIU5fhVwX0S8HxE7gRYll2YUSScCXwTuBIiIDyLiLSrMC2AIOE7SEDAf2EOledEt/VjwTwd2Ndq7s29g5cfP5cBG4JSI2JObXgVOyfXx4jII8foZcBPwcbZPAt6KiIPZbs7p0Hxz+/4cPwhxgHIUuhf4TZ7iukPS8VSWFxHxMvAT4D+UQr8feIp686Ir+rHgV0XSCcADwA0R8XZzW5TPpAN97wtJlwGvRcRTvd6XPjEErAB+FRHLgfcop3AOqSQvRihH50uA04DjmXmfUPpOPxb8l4EzGu1F2TdwJM2hFPvfR8SD2f3f/EhOPr6W/ePFZabH63zga5JepJy+uwD4OeXUxFCOac7p0Hxz+4nAPmZ+HEbtBnZHxMZsr6P8B1BbXnwZ2BkReyPiQ+BBSq7Umhdd0Y8F/0ngzPw2fi7lC5hHerxPXZfnF+8EtkTEbY1NjwCjV1SsAR5u9H87r8pYCezPj/iPAhdJGsmjoouyb0aIiJsjYlFELKb8W/8tIr4JPA5cnsPGxmE0Ppfn+Mj+1Xm1xhLgTOCJaZpG10TEq8AuSZ/NrguB56gsLyinclZKmp/vldE4VJkXXdPrb40Pt1CuPNhO+Ub9ll7vzzGa4+cpH8s3A0/ncinlvONjwA7gr8DCHC/glxmTfwHnNp7rSsqXUS3gO72eWwcx+RLtq3SWUt6YLeBPwHD2z8t2K7cvbfz9LRmfbcBXej2fDuJwDrApc+MhylU21eUF8ANgK/AscDflSptq86IbizIgZmY24PrxlI6ZmR0DLvhmZpVwwTczq4QLvplZJToq+JKWSton6SNJkcu2MWPOkfTimDFXdbbbZmY2VZ0e4d9LuQRqFuWywk2Ua+iva4w5QLl2divlhxAB/HCiJ5Z0TYf7NjAcizbHos2xaHMsJqfTgr8ceAX4kHInu7Mp18HeMDogIrYDZwEnAHcBHwEn5Y8pjsT/gG2ORZtj0eZYtDkWk9BpwZ8DnEw5it+c7V2UW7ICIGkW5YckLeAdyu2P36X8kMTMzKbJ0EQDJO2j3LhorJ82GxERkg73K67vUor8bcA9lCP8d8d5rb3AJxvt9ybav+mgoeH5kx0bB98/0O3X1dAws+bMO+Iv5Lr5upPVo7gM90teTJfx4jyZvIDpz42p5MWRTHG/q8uLIzgQEScfbsOEBT8ixj0Sl3Qj5Vau8yWdDRyk3KjojcawL1CO/Nc3+k4DFgOvj3mtQzspaVNEnDvR/tXAsWhzLNocizbHYnI6PaXzNKXAzwF+Szmt8xnK3Q5HfQt4Cfgf5TavbwKPRcSmDl/bzMymYMIj/Al8g3KVzseU8/RQztW/LWn0lqZ/Bj6d236Uj5/q8HXNzGyKOir4EdGi8QXtGHc31u85iqf/9VH8zaByLNocizbHos2xmATfLdPMrBK+tYKZWSVc8M3MKuGCb2ZWCRd8M7NKuOCbmVXCBd/MrBIu+GZmlfg/mTdCD6et+OwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRI_AZtROxP0",
        "outputId": "9f2bafae-ddd9-40d9-8de6-3f0263cb4423"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([\"change number node recognition rate defined relative frequency\"]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "sorted_items"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7360, 0.6332528462762741),\n",
              " (6110, 0.48592082711420476),\n",
              " (3282, 0.2823916239785495),\n",
              " (5971, 0.2677155552680803),\n",
              " (7348, 0.2262665323411533),\n",
              " (7490, 0.2242783939550881),\n",
              " (1179, 0.1963759835498492),\n",
              " (7270, 0.17762842408345383),\n",
              " (2062, 0.15648471115689686),\n",
              " (6087, 0.12391506848540515)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxhq2LiEOxD3",
        "outputId": "b599803d-c46c-4ec0-a97b-e1049499fddb"
      },
      "source": [
        "coo_matrix = tf_idf_vector.tocoo()\n",
        "#list(zip(coo_matrix.col, coo_matrix.data))\n",
        "coo_matrix"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4EwdqH_Gm0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04454eb-34ed-4c95-814c-98ead96d68fd"
      },
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(txt, top_N=10):\n",
        "\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  # Get tf idf\n",
        "  count_vector = cv.transform([txt]) \n",
        "  tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
        "\n",
        "  # get scores\n",
        "  scores = np.asarray(tf_idf_vector.sum(axis=0)).ravel()\n",
        "  scores_rounded = list(np.around(scores, 3))\n",
        "\n",
        "  # Get keyword scores\n",
        "  keyword_score =  list(zip(feature_names, scores_rounded))\n",
        "  keyword_score.sort(key=lambda element: element[1], reverse=True)\n",
        "  sorted_keyword_score = keyword_score[:top_N]\n",
        "  # ------------------\n",
        "  \n",
        "  return sorted_keyword_score"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahvTo9gjSWCW",
        "outputId": "f886f5b1-fb00-4451-a323-8a098fc3ae03"
      },
      "source": [
        "get_keywords(txt=\"change number node recognition rate defined relative frequency\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('recognition rate', 0.633),\n",
              " ('number node', 0.486),\n",
              " ('frequency', 0.282),\n",
              " ('node', 0.268),\n",
              " ('recognition', 0.226),\n",
              " ('relative', 0.224),\n",
              " ('change', 0.196),\n",
              " ('rate', 0.178),\n",
              " ('defined', 0.156),\n",
              " ('number', 0.124)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9hzsg0CUQZ_"
      },
      "source": [
        "### Compare Raw Counts to Tf-IDF approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot6GLVIiUPqj"
      },
      "source": [
        "df[\"Top_N_TF-IDF\"] = df[\"preproc_text\"].apply(get_keywords, top_N=10)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4sydV20UjvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cf5fa0-40e1-4820-8f18-cbcc9aadbaf2"
      },
      "source": [
        "comparator = list(df[['Top N', 'Top_N_TF-IDF']].iloc[1].values)\n",
        "\n",
        "print(comparator[0])\n",
        "print(comparator[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cell', 51), ('network', 42), ('cortical', 32), ('synapsis', 26), ('activity', 19), ('mean', 17), ('field', 17), ('eye', 17), ('layer', 13), ('single', 12)]\n",
            "[('cell', 0.456), ('cortical', 0.365), ('synapsis', 0.342), ('mean field', 0.223), ('eye', 0.211), ('network', 0.197), ('lgn', 0.175), ('activity', 0.17), ('mean field approximation', 0.147), ('field approximation', 0.146)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDLV8yILNNS"
      },
      "source": [
        "### Question 1.5: Find an example where there is a noticeable difference between tf-idf and raw counts? Justify which method you would choose yourself (there is no bad and good answer here) [written] (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparator = list(df[['Top N', 'Top_N_TF-IDF']].iloc[4].values)\n",
        "\n",
        "print(f'Raw counts: {comparator[0]}')\n",
        "print(f'tf-idf : {comparator[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFpjlzCP7D7W",
        "outputId": "9d9df393-845a-464d-c11c-e3719ca34c30"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw counts: [('ensemble', 56), ('network', 52), ('error', 51), ('generalization', 40), ('set', 27), ('ambiguity', 26), ('weight', 26), ('learning', 22), ('training', 22), ('example', 22)]\n",
            "tf-idf : [('ensemble', 0.525), ('generalization error', 0.356), ('ambiguity', 0.302), ('generalization', 0.232), ('network', 0.21), ('error', 0.203), ('active learning', 0.195), ('cross validation', 0.14), ('active', 0.136), ('individual', 0.125)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in this example (text 4) that the general words look similar. Although, upon inspection we realise that the words from `Raw counts` lack context and meaning for example we see `learning` as a keyword but what learning are we talking about machine learning, active learning, ...? On the other end, tf-idf is able to classify `active learning` as one of its keywords which actually gives more information on the contents of the article! We can further see those differences with `error` and `generalization error`.\n",
        "\n",
        "Overall at least on this example, tf-idf seems to give way more context than raw counts and seems to be a much better choice in this particular case.\n",
        "\n",
        "Although we can generalise this observersation as: raw counts are the number of times a term appears in a document which appear relatively naive. tf-idf is a measure of how important a term is to a document in a collection of documents which seems to be a more complete overview. It is the product of the term's raw count and a document frequency weight. tf-idf is thus better because it takes into account how often a term appears in a document."
      ],
      "metadata": {
        "id": "Rr2OVkoZ7a__"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mes_RnLNVXBX"
      },
      "source": [
        "## 2. KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7iWThDYXlJY"
      },
      "source": [
        "### 2.0. Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Ji149nXmUU"
      },
      "source": [
        "%%capture\n",
        "pip install keybert"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-kXWUGKXqIB"
      },
      "source": [
        "%%capture\n",
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of learning a function that\n",
        "         maps an input to an output based on example input-output pairs. It infers a\n",
        "         function from labeled training data consisting of a set of training examples.\n",
        "         In supervised learning, each example is a pair consisting of an input object\n",
        "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
        "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
        "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
        "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
        "         the learning algorithm to generalize from the training data to unseen situations in a \n",
        "         'reasonable' way (see inductive bias).\n",
        "      \"\"\"\n",
        "kw_model = KeyBERT()\n",
        "keywords = kw_model.extract_keywords(doc)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNrYae2jYDqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9234f24c-d334-4a9d-9bf3-553307ac9b5b"
      },
      "source": [
        "keywords"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supervised', 0.6676),\n",
              " ('labeled', 0.4896),\n",
              " ('learning', 0.4813),\n",
              " ('training', 0.4134),\n",
              " ('labels', 0.3947)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJs_f30AYmKN"
      },
      "source": [
        "### Question 2.1. Apply KeyBERT to the a sample of the dataset [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GisGAL0pIDy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "6bce4d3f-f9b0-44cc-b0d2-41de3438032c"
      },
      "source": [
        "df_ = df.sample(100)\n",
        "df_.sample(1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "2263  3054  2006  Clustering appearance and shape by learning ji...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "2263        NaN  3054-clustering-appearance-and-shape-by-learni...   \n",
              "\n",
              "              abstract                                         paper_text  \\\n",
              "2263  Abstract Missing  Clustering appearance and shape by learning ji...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "2263  clustering appearance shape learning jigsaw an...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "2263  [(jigsaw, 120), (model, 71), (piece, 51), (pat...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \n",
              "2263  [(piece, 0.449), (epitome, 0.414), (patch, 0.3...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2233370-c78a-4e0e-aeec-0e801697ca1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2263</th>\n",
              "      <td>3054</td>\n",
              "      <td>2006</td>\n",
              "      <td>Clustering appearance and shape by learning ji...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3054-clustering-appearance-and-shape-by-learni...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Clustering appearance and shape by learning ji...</td>\n",
              "      <td>clustering appearance shape learning jigsaw an...</td>\n",
              "      <td>[(jigsaw, 120), (model, 71), (piece, 51), (pat...</td>\n",
              "      <td>[(piece, 0.449), (epitome, 0.414), (patch, 0.3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2233370-c78a-4e0e-aeec-0e801697ca1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2233370-c78a-4e0e-aeec-0e801697ca1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2233370-c78a-4e0e-aeec-0e801697ca1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN3Dlu11YwLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c947487-4959-4cfb-f3d4-a0c1d4b27d07"
      },
      "source": [
        "%%time\n",
        "%%capture\n",
        "\n",
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "def get_key_bert_keywords(text):\n",
        "  keywords = kw_model.extract_keywords(text)\n",
        "\n",
        "  return keywords\n",
        "\n",
        "df_[\"Top_N_KeyBERT_1\"] = df_[\"preproc_text\"].apply(get_key_bert_keywords)\n",
        "# ------------------"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 41s, sys: 17.1 s, total: 3min 58s\n",
            "Wall time: 3min 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "ycwvmMoS4i_6",
        "outputId": "3a5df66e-f13b-48e5-ee0f-3718760fd9ae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "2628  3383  2008                                   Spectral Hashing   \n",
              "5828  6273  2016  Fundamental Limits of Budget-Fidelity Trade-of...   \n",
              "5844  6288  2016  Split LBI: An Iterative Regularization Path wi...   \n",
              "7153   916  1994  An Actor/Critic Algorithm that is Equivalent t...   \n",
              "4316  4906  2013  Convex Calibrated Surrogates for Low-Rank Loss...   \n",
              "...    ...   ...                                                ...   \n",
              "5570  6040  2016  Stochastic Gradient Methods for Distributional...   \n",
              "1784  2620  2004  Learning Efficient Auditory Codes Using Spikes...   \n",
              "4179  4783  2012  Optimal Neural Tuning Curves for Arbitrary Sti...   \n",
              "3653  4308  2011      Statistical Tests for Optimization Efficiency   \n",
              "5575  6045  2016  Learning shape correspondence with anisotropic...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "2628        NaN                          3383-spectral-hashing.pdf   \n",
              "5828     Poster  6273-fundamental-limits-of-budget-fidelity-tra...   \n",
              "5844     Poster  6288-split-lbi-an-iterative-regularization-pat...   \n",
              "7153        NaN  916-an-actorcritic-algorithm-that-is-equivalen...   \n",
              "4316  Spotlight  4906-convex-calibrated-surrogates-for-low-rank...   \n",
              "...         ...                                                ...   \n",
              "5570     Poster  6040-stochastic-gradient-methods-for-distribut...   \n",
              "1784        NaN  2620-learning-efficient-auditory-codes-using-s...   \n",
              "4179        NaN  4783-optimal-neural-tuning-curves-for-arbitrar...   \n",
              "3653        NaN  4308-statistical-tests-for-optimization-effici...   \n",
              "5575     Poster  6045-learning-shape-correspondence-with-anisot...   \n",
              "\n",
              "                                               abstract  \\\n",
              "2628  Semantic hashing seeks compact binary codes of...   \n",
              "5828  Digital crowdsourcing (CS) is a modern approac...   \n",
              "5844  An iterative regularization path with structur...   \n",
              "7153                                   Abstract Missing   \n",
              "4316  The design of convex, calibrated surrogate los...   \n",
              "...                                                 ...   \n",
              "5570  We develop efficient solution methods for a ro...   \n",
              "1784                                   Abstract Missing   \n",
              "4179  In this work we study how the stimulus distrib...   \n",
              "3653  Learning problems such as logistic regression ...   \n",
              "5575  Convolutional neural networks have achieved ex...   \n",
              "\n",
              "                                             paper_text  \\\n",
              "2628  Spectral Hashing\\n\\n3\\n\\nYair Weiss1,3\\nSchool...   \n",
              "5828  Fundamental Limits of Budget-Fidelity Trade-of...   \n",
              "5844  Split LBI: An Iterative Regularization Path wi...   \n",
              "7153  An Actor/Critic Algorithm that\\nEquivalent to ...   \n",
              "4316  Convex Calibrated Surrogates for Low-Rank Loss...   \n",
              "...                                                 ...   \n",
              "5570  Stochastic Gradient Methods for Distributional...   \n",
              "1784  Learning efficient auditory codes using spikes...   \n",
              "4179  Optimal Neural Tuning Curves for Arbitrary\\nSt...   \n",
              "3653  Statistical Tests for Optimization Efficiency\\...   \n",
              "5575  Learning shape correspondence with\\nanisotropi...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "2628  spectral hashing yair wei school science hebre...   \n",
              "5828  fundamental limit budget fidelity trade label ...   \n",
              "5844  split lbi iterative regularization path struct...   \n",
              "7153  actor critic algorithm equivalent learning rob...   \n",
              "4316  convex calibrated surrogate low rank loss matr...   \n",
              "...                                                 ...   \n",
              "5570  stochastic gradient method distributionally ro...   \n",
              "1784  learning efficient auditory code spike predict...   \n",
              "4179  optimal neural tuning curve arbitrary stimulus...   \n",
              "3653  statistical test optimization efficiency levi ...   \n",
              "5575  learning shape correspondence anisotropic conv...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "2628  [(code, 55), (bit, 54), (spectral, 38), (hashi...   \n",
              "5828  [(worker, 59), (query, 58), (crowdsourcing, 41...   \n",
              "5844  [(lasso, 44), (path, 30), (lbi, 29), (split, 2...   \n",
              "7153  [(actor, 41), (critic, 41), (algorithm, 38), (...   \n",
              "4316  [(surrogate, 76), (loss, 66), (calibrated, 45)...   \n",
              "...                                                 ...   \n",
              "5570  [(algorithm, 37), (robust, 31), (problem, 27),...   \n",
              "1784  [(kernel, 54), (function, 47), (signal, 40), (...   \n",
              "4179  [(tuning, 44), (information, 43), (curve, 41),...   \n",
              "3653  [(algorithm, 32), (loss, 29), (learning, 27), ...   \n",
              "5575  [(shape, 58), (correspondence, 52), (intrinsic...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \\\n",
              "2628  [(hashing, 0.386), (bit, 0.341), (eigenfunctio...   \n",
              "5828  [(worker, 0.525), (crowdsourcing, 0.42), (quer...   \n",
              "5844  [(lasso, 0.507), (consistency, 0.255), (path, ...   \n",
              "7153  [(critic, 0.514), (actor, 0.494), (actor criti...   \n",
              "4316  [(surrogate, 0.568), (pred, 0.443), (calibrate...   \n",
              "...                                                 ...   \n",
              "5570  [(regret, 0.268), (robust, 0.246), (convex, 0....   \n",
              "1784  [(spike, 0.352), (kernel, 0.326), (kernel func...   \n",
              "4179  [(tuning curve, 0.505), (tuning, 0.314), (curv...   \n",
              "3653  [(sgd, 0.407), (loss, 0.225), (regularized, 0....   \n",
              "5575  [(correspondence, 0.412), (shape, 0.354), (geo...   \n",
              "\n",
              "                                        Top_N_KeyBERT_1  \n",
              "2628  [(hashing, 0.5136), (codewords, 0.4106), (retr...  \n",
              "5828  [(crowdsourcing, 0.5045), (crowdsourcer, 0.495...  \n",
              "5844  [(lasso, 0.4529), (regularization, 0.422), (re...  \n",
              "7153  [(reinforcement, 0.4622), (critic, 0.41), (ada...  \n",
              "4316  [(convex, 0.3066), (optimization, 0.3048), (mu...  \n",
              "...                                                 ...  \n",
              "5570  [(optimizer, 0.3316), (optimization, 0.3129), ...  \n",
              "1784  [(auditory, 0.5245), (cochlear, 0.5083), (coch...  \n",
              "4179  [(neuron, 0.3313), (neuronal, 0.3308), (optimi...  \n",
              "3653  [(optimizers, 0.4044), (optimize, 0.3705), (op...  \n",
              "5575  [(shapenets, 0.4044), (recognition, 0.3946), (...  \n",
              "\n",
              "[100 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f5a7a2e-c707-4c7c-8598-3e122708feff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "      <th>Top_N_KeyBERT_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2628</th>\n",
              "      <td>3383</td>\n",
              "      <td>2008</td>\n",
              "      <td>Spectral Hashing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3383-spectral-hashing.pdf</td>\n",
              "      <td>Semantic hashing seeks compact binary codes of...</td>\n",
              "      <td>Spectral Hashing\\n\\n3\\n\\nYair Weiss1,3\\nSchool...</td>\n",
              "      <td>spectral hashing yair wei school science hebre...</td>\n",
              "      <td>[(code, 55), (bit, 54), (spectral, 38), (hashi...</td>\n",
              "      <td>[(hashing, 0.386), (bit, 0.341), (eigenfunctio...</td>\n",
              "      <td>[(hashing, 0.5136), (codewords, 0.4106), (retr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5828</th>\n",
              "      <td>6273</td>\n",
              "      <td>2016</td>\n",
              "      <td>Fundamental Limits of Budget-Fidelity Trade-of...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>6273-fundamental-limits-of-budget-fidelity-tra...</td>\n",
              "      <td>Digital crowdsourcing (CS) is a modern approac...</td>\n",
              "      <td>Fundamental Limits of Budget-Fidelity Trade-of...</td>\n",
              "      <td>fundamental limit budget fidelity trade label ...</td>\n",
              "      <td>[(worker, 59), (query, 58), (crowdsourcing, 41...</td>\n",
              "      <td>[(worker, 0.525), (crowdsourcing, 0.42), (quer...</td>\n",
              "      <td>[(crowdsourcing, 0.5045), (crowdsourcer, 0.495...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5844</th>\n",
              "      <td>6288</td>\n",
              "      <td>2016</td>\n",
              "      <td>Split LBI: An Iterative Regularization Path wi...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>6288-split-lbi-an-iterative-regularization-pat...</td>\n",
              "      <td>An iterative regularization path with structur...</td>\n",
              "      <td>Split LBI: An Iterative Regularization Path wi...</td>\n",
              "      <td>split lbi iterative regularization path struct...</td>\n",
              "      <td>[(lasso, 44), (path, 30), (lbi, 29), (split, 2...</td>\n",
              "      <td>[(lasso, 0.507), (consistency, 0.255), (path, ...</td>\n",
              "      <td>[(lasso, 0.4529), (regularization, 0.422), (re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7153</th>\n",
              "      <td>916</td>\n",
              "      <td>1994</td>\n",
              "      <td>An Actor/Critic Algorithm that is Equivalent t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>916-an-actorcritic-algorithm-that-is-equivalen...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>An Actor/Critic Algorithm that\\nEquivalent to ...</td>\n",
              "      <td>actor critic algorithm equivalent learning rob...</td>\n",
              "      <td>[(actor, 41), (critic, 41), (algorithm, 38), (...</td>\n",
              "      <td>[(critic, 0.514), (actor, 0.494), (actor criti...</td>\n",
              "      <td>[(reinforcement, 0.4622), (critic, 0.41), (ada...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4316</th>\n",
              "      <td>4906</td>\n",
              "      <td>2013</td>\n",
              "      <td>Convex Calibrated Surrogates for Low-Rank Loss...</td>\n",
              "      <td>Spotlight</td>\n",
              "      <td>4906-convex-calibrated-surrogates-for-low-rank...</td>\n",
              "      <td>The design of convex, calibrated surrogate los...</td>\n",
              "      <td>Convex Calibrated Surrogates for Low-Rank Loss...</td>\n",
              "      <td>convex calibrated surrogate low rank loss matr...</td>\n",
              "      <td>[(surrogate, 76), (loss, 66), (calibrated, 45)...</td>\n",
              "      <td>[(surrogate, 0.568), (pred, 0.443), (calibrate...</td>\n",
              "      <td>[(convex, 0.3066), (optimization, 0.3048), (mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>6040</td>\n",
              "      <td>2016</td>\n",
              "      <td>Stochastic Gradient Methods for Distributional...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>6040-stochastic-gradient-methods-for-distribut...</td>\n",
              "      <td>We develop efficient solution methods for a ro...</td>\n",
              "      <td>Stochastic Gradient Methods for Distributional...</td>\n",
              "      <td>stochastic gradient method distributionally ro...</td>\n",
              "      <td>[(algorithm, 37), (robust, 31), (problem, 27),...</td>\n",
              "      <td>[(regret, 0.268), (robust, 0.246), (convex, 0....</td>\n",
              "      <td>[(optimizer, 0.3316), (optimization, 0.3129), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>2620</td>\n",
              "      <td>2004</td>\n",
              "      <td>Learning Efficient Auditory Codes Using Spikes...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2620-learning-efficient-auditory-codes-using-s...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning efficient auditory codes using spikes...</td>\n",
              "      <td>learning efficient auditory code spike predict...</td>\n",
              "      <td>[(kernel, 54), (function, 47), (signal, 40), (...</td>\n",
              "      <td>[(spike, 0.352), (kernel, 0.326), (kernel func...</td>\n",
              "      <td>[(auditory, 0.5245), (cochlear, 0.5083), (coch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4179</th>\n",
              "      <td>4783</td>\n",
              "      <td>2012</td>\n",
              "      <td>Optimal Neural Tuning Curves for Arbitrary Sti...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4783-optimal-neural-tuning-curves-for-arbitrar...</td>\n",
              "      <td>In this work we study how the stimulus distrib...</td>\n",
              "      <td>Optimal Neural Tuning Curves for Arbitrary\\nSt...</td>\n",
              "      <td>optimal neural tuning curve arbitrary stimulus...</td>\n",
              "      <td>[(tuning, 44), (information, 43), (curve, 41),...</td>\n",
              "      <td>[(tuning curve, 0.505), (tuning, 0.314), (curv...</td>\n",
              "      <td>[(neuron, 0.3313), (neuronal, 0.3308), (optimi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3653</th>\n",
              "      <td>4308</td>\n",
              "      <td>2011</td>\n",
              "      <td>Statistical Tests for Optimization Efficiency</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4308-statistical-tests-for-optimization-effici...</td>\n",
              "      <td>Learning problems such as logistic regression ...</td>\n",
              "      <td>Statistical Tests for Optimization Efficiency\\...</td>\n",
              "      <td>statistical test optimization efficiency levi ...</td>\n",
              "      <td>[(algorithm, 32), (loss, 29), (learning, 27), ...</td>\n",
              "      <td>[(sgd, 0.407), (loss, 0.225), (regularized, 0....</td>\n",
              "      <td>[(optimizers, 0.4044), (optimize, 0.3705), (op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5575</th>\n",
              "      <td>6045</td>\n",
              "      <td>2016</td>\n",
              "      <td>Learning shape correspondence with anisotropic...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>6045-learning-shape-correspondence-with-anisot...</td>\n",
              "      <td>Convolutional neural networks have achieved ex...</td>\n",
              "      <td>Learning shape correspondence with\\nanisotropi...</td>\n",
              "      <td>learning shape correspondence anisotropic conv...</td>\n",
              "      <td>[(shape, 58), (correspondence, 52), (intrinsic...</td>\n",
              "      <td>[(correspondence, 0.412), (shape, 0.354), (geo...</td>\n",
              "      <td>[(shapenets, 0.4044), (recognition, 0.3946), (...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows  11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f5a7a2e-c707-4c7c-8598-3e122708feff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f5a7a2e-c707-4c7c-8598-3e122708feff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f5a7a2e-c707-4c7c-8598-3e122708feff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYDNwWa5ZiTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "86d4a778-be49-4067-deb0-a36fd6f7f80d"
      },
      "source": [
        "# TODO: compare the same paper example across the 3 methods \n",
        "\n",
        "idx_focus = 1271 \n",
        "\n",
        "print(f'Keybert: {df_.loc[idx_focus, \"Top_N_KeyBERT_1\"]}')\n",
        "print(f'TF-IDF: {df_.loc[idx_focus, \"Top_N_TF-IDF\"]}')\n",
        "print(f'Top N: {df_.loc[idx_focus, \"Top N\"]}') "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1271",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e0b067a80177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0midx_focus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1271\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Keybert: {df_.loc[idx_focus, \"Top_N_KeyBERT_1\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'TF-IDF: {df_.loc[idx_focus, \"Top_N_TF-IDF\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top N: {df_.loc[idx_focus, \"Top N\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3774\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3776\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1271"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GQQGMTZgrR"
      },
      "source": [
        "### Question 1.6. Comparison of multilple techniques [written] (4 points)\n",
        "\n",
        "1. Draw a table of the solution, the quality score that you defined and the time taken to find keywords across a sample of 1000 of the original dataset. \n",
        "2. Can you think of tweaks to reduce time to compute? If yes, add an additional column to the above table with your proposed tweaks.\n",
        "3. Based on the above table and  lecture 1, what do you think is the most appropriate solution for keywords extraction? Why? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOOOOOOOOOO"
      ],
      "metadata": {
        "id": "ObtV0t6W5Wqe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhS_Y6qgqW4Z"
      },
      "source": [
        "# Part 2. Word Vectors (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kedMB9f-qsnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0cb311-8464-47b3-d040-f69f36858bab"
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5S4yJ_ZrHAo"
      },
      "source": [
        "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
        "\n",
        "Note on Terminology: The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcD1SUIrP_m"
      },
      "source": [
        "## Count-Based Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvm6lbSsFD0"
      },
      "source": [
        "Most word vector models start from the following idea:\n",
        "\n",
        "You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
        "\n",
        "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMxbozcslLA"
      },
      "source": [
        "## Plotting Co-Occurrence Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OO_oowsrK2"
      },
      "source": [
        "\n",
        "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a read_corpus function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds <START> and <END> tokens to each of the documents, and lowercases words. You do not have to perform any other kind of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTQwympsqDq"
      },
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrwL93ns1Qy"
      },
      "source": [
        "Let's have a look what these documents are like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eZvFI3Qs0x4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d414cd1e-19f9-4ddb-c8ae-6d51750a0e61"
      },
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
            "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
            "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
            "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
            "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
            "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
            "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
            "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
            "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
            "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
            "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
            "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
            "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
            "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
            "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
            "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
            "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
            "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
            "  'they', 'noted', '.', '<END>'],\n",
            " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
            "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
            "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
            "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
            "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
            "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
            "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
            "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
            "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
            "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
            "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
            "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
            "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
            "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
            "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
            "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
            "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
            "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
            "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
            "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
            "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
            "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
            "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
            "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
            "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
            "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
            "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
            "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
            "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
            "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
            "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
            "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
            "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
            "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
            "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
            "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
            "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
            "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
            "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
            "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
            "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
            "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
            "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
            "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
            "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
            "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
            "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
            "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
            "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
            "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
            "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
            "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
            "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
            "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
            "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
            "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
            "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
            "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
            "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
            "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
            "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
            "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
            "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
            "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
            "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
            "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
            "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
            "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
            "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
            "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
            "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
            "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
            "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
            "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
            "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
            "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
            "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
            "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
            "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
            "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
            "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
            "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
            "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
            "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
            "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
            "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
            "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
            "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
            "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
            "  '<END>'],\n",
            " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
            "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
            "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
            "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
            "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
            "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
            "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
            "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
            "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
            "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
            "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
            "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
            "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
            "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
            "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
            "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
            "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
            "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
            "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
            "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
            "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNKy6j3as7xJ"
      },
      "source": [
        "### Question 2.1: Implement distinct_words [code] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIgkQ47otdqZ"
      },
      "source": [
        "Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with for loops, but it's more efficient to do it with Python list comprehensions. In particular, this may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's more information.\n",
        "\n",
        "Your returned corpus_words should be sorted. You can use python's sorted function for this.\n",
        "\n",
        "You may find it useful to use Python sets to remove duplicate words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTIH5vFetgjD"
      },
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents - eg [[\"hey\", \"I\", \"am\", \"toto\"], [\"hey\", \"I\", \"am\", \"tata\"]]\n",
        "        Return:\n",
        "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.w\n",
        "    #flattened_corpus = np.array(corpus, dtype=object).flatten()\n",
        "    flattened_corpus = np.concatenate(corpus).ravel()\n",
        "\n",
        "    #print(set(map(tuple, flattened_corpus)))\n",
        "    unique_flat_corpus = list(set(flattened_corpus))\n",
        "    \n",
        "    corpus_words = sorted(unique_flat_corpus)\n",
        "    num_corpus_words = len(corpus_words)\n",
        "    # ------------------\n",
        "\n",
        "    return corpus_words, num_corpus_words"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZX4dH8stmYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65331c4b-bed7-4180-e78b-95f9b5e092ff"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fD2hYr3fw8"
      },
      "source": [
        "### Question 2.2: Implement compute_co_occurrence_matrix [code] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4MLCIa3lKw"
      },
      "source": [
        "Write a method that constructs a co-occurrence matrix for a certain window-size  n  (with a default of 4), considering words  n  before and  n  after the word in the center of the window. Here, we start to use numpy (np) to represent vectors, matrices, and tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz5vrGb43lbA"
      },
      "source": [
        "from nltk.tag.mapping import join\n",
        "from collections import defaultdict\n",
        "from collections import Counter \n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2ind = {}\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    corpus_words, num_corpus_words  = distinct_words(corpus)\n",
        "\n",
        "    M = np.zeros((num_corpus_words, num_corpus_words))\n",
        "\n",
        "    word2ind = dict(zip(corpus_words, range(num_corpus_words)))\n",
        "\n",
        "    for sentence in corpus:\n",
        "      l = len(sentence)\n",
        "      for i in range(l):\n",
        "        # Get max index for iter\n",
        "        maximum = min(i + window_size + 1, l)\n",
        "        \n",
        "        # Get the index of center word in M.\n",
        "        center_word_index = word2ind[sentence[i]]\n",
        "\n",
        "        for j in range(i, maximum):\n",
        "          if (i == j):\n",
        "            continue\n",
        "\n",
        "          curr_word_index = word2ind[sentence[j]]\n",
        "\n",
        "          # Setting both before and after at once instead of looking through\n",
        "          # bigger loops!\n",
        "          M[curr_word_index][center_word_index] = 1\n",
        "          M[center_word_index][curr_word_index] = 1\n",
        "\n",
        "    # ------------------\n",
        "\n",
        "    return M, word2ind"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guUdCsM2BUuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f6645b-6564-4707-8557-aba1f9d95200"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCC24T0WPyI2"
      },
      "source": [
        "### Question 2.3: Implement reduce_to_k_dim [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9XXG-WP2dZ"
      },
      "source": [
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use sklearn.decomposition.TruncatedSVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jfqvOUOP8R6"
      },
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    # Random state set for reproducability \n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)#, random_state=42)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "\n",
        "    # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGeaWNuRAnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c3dd0f-b0f5-47d9-a87a-a2f2f6948f24"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 10 words...\n",
            "Done.\n",
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iTgMaquRQKB"
      },
      "source": [
        "### Question 2.4: Implement plot_embeddings [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H629WACPRTg2"
      },
      "source": [
        "Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).\n",
        "\n",
        "For this example, you may find it useful to adapt this code. In the future, a good way to make a plot is to look at the Matplotlib gallery, find a plot that looks somewhat like what you want, and adapt the code they give."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "metadata": {
        "id": "VYglHcvXNRZS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMfaxKfERT1P"
      },
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    word_indexes_to_plot = [word2ind[word] for word in words]\n",
        "    values_to_plot = [M_reduced[word] for word in word_indexes_to_plot]\n",
        "\n",
        "    x = [item[1] for item in values_to_plot]\n",
        "    y = [item[0] for item in values_to_plot]\n",
        "\n",
        "    plot = plt.scatter(x, y)\n",
        "    plt.setp(plot, color='#7E1E9C', linewidth=10.0, alpha=0.5)\n",
        "\n",
        "    for i, text in enumerate(words):\n",
        "      plt.annotate(text, (x[i], y[i]))\n",
        "\n",
        "    plt.show()    \n",
        "    # ------------------"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ5sOXmXRYOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "2d7b1c78-25b5-408e-e876-f0356e24d1f8"
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below. \n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Outputted Plot:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 397.192756 248.518125\" width=\"397.192756pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 397.192756 248.518125 \nL 397.192756 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 44.845313 224.64 \nL 379.645313 224.64 \nL 379.645313 7.2 \nL 44.845313 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma070266eda\" style=\"stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\"/>\n    </defs>\n    <g clip-path=\"url(#p63e506e394)\">\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"364.427131\" xlink:href=\"#ma070266eda\" y=\"17.083636\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"60.063494\" xlink:href=\"#ma070266eda\" y=\"214.756364\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"60.063494\" xlink:href=\"#ma070266eda\" y=\"17.083636\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"364.427131\" xlink:href=\"#ma070266eda\" y=\"214.756364\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"212.245313\" xlink:href=\"#ma070266eda\" y=\"115.92\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"maaf9778c32\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.063494\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.00 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(44.740838 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.108949\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(82.786293 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.154403\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.50 -->\n      <g transform=\"translate(120.831747 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.199858\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(158.877202 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"212.245313\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.00 -->\n      <g transform=\"translate(201.1125 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.290767\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.25 -->\n      <g transform=\"translate(239.157955 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.336222\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.50 -->\n      <g transform=\"translate(277.203409 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.381676\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.75 -->\n      <g transform=\"translate(315.248864 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.427131\" xlink:href=\"#maaf9778c32\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.00 -->\n      <g transform=\"translate(353.294318 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mae8ed8cd92\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"214.756364\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"190.047273\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.75 -->\n      <g transform=\"translate(7.2 193.846491)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"165.338182\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 169.137401)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"140.629091\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 144.42831)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.00 -->\n      <g transform=\"translate(15.579688 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"91.210909\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.25 -->\n      <g transform=\"translate(15.579688 95.010128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"66.501818\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.50 -->\n      <g transform=\"translate(15.579688 70.301037)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"41.792727\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.75 -->\n      <g transform=\"translate(15.579688 45.591946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mae8ed8cd92\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 1.00 -->\n      <g transform=\"translate(15.579688 20.882855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 44.845313 224.64 \nL 44.845313 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 379.645313 224.64 \nL 379.645313 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 44.845312 224.64 \nL 379.645313 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 44.845312 7.2 \nL 379.645313 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- test1 -->\n    <defs>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n    </defs>\n    <g transform=\"translate(364.427131 17.083636)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n   <g id=\"text_20\">\n    <!-- test2 -->\n    <g transform=\"translate(60.063494 214.756364)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n   <g id=\"text_21\">\n    <!-- test3 -->\n    <defs>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n    </defs>\n    <g transform=\"translate(60.063494 17.083636)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-51\"/>\n    </g>\n   </g>\n   <g id=\"text_22\">\n    <!-- test4 -->\n    <defs>\n     <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n    </defs>\n    <g transform=\"translate(364.427131 214.756364)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-52\"/>\n    </g>\n   </g>\n   <g id=\"text_23\">\n    <!-- test5 -->\n    <g transform=\"translate(212.245313 115.92)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"192.041016\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p63e506e394\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"44.845313\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRb0HnWqVCDL"
      },
      "source": [
        "### Question 2.5: Co-Occurrence Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfAWdLFVFx-"
      },
      "source": [
        "Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" (oil) corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). Note: The line of code below that does the normalizing uses the NumPy concept of broadcasting. If you don't know about broadcasting, check out Computation on Arrays: Broadcasting by Jake VanderPlas.\n",
        "\n",
        "Run the below cell to produce the plot. It'll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? Note: \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3d0UK2iVFDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "68889ca8-b05b-4e0e-d0b4-8b46ae60983f"
      },
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 8185 words...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m1e657e47b3\" style=\"stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\"/>\n    </defs>\n    <g clip-path=\"url(#pabf69199f3)\">\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"150.244472\" xlink:href=\"#m1e657e47b3\" y=\"17.083636\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"175.825154\" xlink:href=\"#m1e657e47b3\" y=\"20.601037\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"119.329527\" xlink:href=\"#m1e657e47b3\" y=\"20.078371\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"177.519552\" xlink:href=\"#m1e657e47b3\" y=\"21.026437\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"174.429676\" xlink:href=\"#m1e657e47b3\" y=\"20.268705\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"88.224549\" xlink:href=\"#m1e657e47b3\" y=\"31.156438\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"356.047443\" xlink:href=\"#m1e657e47b3\" y=\"214.756364\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"151.335095\" xlink:href=\"#m1e657e47b3\" y=\"17.122731\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"168.182381\" xlink:href=\"#m1e657e47b3\" y=\"18.980108\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"51.683807\" xlink:href=\"#m1e657e47b3\" y=\"54.815695\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m15e4bb05d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.076347\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(56.93494 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.770049\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(95.628643 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.463752\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.0 -->\n      <g transform=\"translate(138.512189 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.157454\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.1 -->\n      <g transform=\"translate(177.205892 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.851157\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.2 -->\n      <g transform=\"translate(215.899594 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.54486\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(254.593297 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.238562\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(293.287 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.932265\" xlink:href=\"#m15e4bb05d9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(331.980702 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m963596a529\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"215.507325\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.84 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(7.2 219.306544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"190.696962\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.86 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(7.2 194.496181)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"165.886599\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.88 -->\n      <g transform=\"translate(7.2 169.685817)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"141.076235\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.90 -->\n      <defs>\n       <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      </defs>\n      <g transform=\"translate(7.2 144.875454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"116.265872\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.92 -->\n      <g transform=\"translate(7.2 120.065091)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"91.455509\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.94 -->\n      <g transform=\"translate(7.2 95.254727)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"66.645145\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.96 -->\n      <g transform=\"translate(7.2 70.444364)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"41.834782\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.98 -->\n      <g transform=\"translate(7.2 45.634001)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m963596a529\" y=\"17.024419\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 20.823637)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- barrels -->\n    <defs>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n    </defs>\n    <g transform=\"translate(150.244472 17.083636)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"124.755859\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"164.119141\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"202.982422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"264.505859\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"292.289062\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"text_19\">\n    <!-- bpd -->\n    <defs>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n    </defs>\n    <g transform=\"translate(175.825154 20.601037)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"126.953125\" xlink:href=\"#DejaVuSans-100\"/>\n    </g>\n   </g>\n   <g id=\"text_20\">\n    <!-- ecuador -->\n    <defs>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n    </defs>\n    <g transform=\"translate(119.329527 20.078371)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"61.523438\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"116.503906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"179.882812\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"241.162109\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"304.638672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.820312\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n   <g id=\"text_21\">\n    <!-- energy -->\n    <defs>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n    </defs>\n    <g transform=\"translate(177.519552 21.026437)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"61.523438\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.902344\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"186.425781\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"225.789062\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"289.265625\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"text_22\">\n    <!-- industry -->\n    <defs>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(174.429676 20.268705)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"91.162109\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"218.017578\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"270.117188\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"309.326172\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"350.439453\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"text_23\">\n    <!-- kuwait -->\n    <defs>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n    </defs>\n    <g transform=\"translate(88.224549 31.156438)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"54.785156\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"118.164062\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"199.951172\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"261.230469\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"289.013672\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n   <g id=\"text_24\">\n    <!-- oil -->\n    <g transform=\"translate(356.047443 214.756364)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"61.181641\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n   <g id=\"text_25\">\n    <!-- output -->\n    <g transform=\"translate(151.335095 17.122731)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"61.181641\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"124.560547\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"163.769531\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"227.246094\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"290.625\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n   <g id=\"text_26\">\n    <!-- petroleum -->\n    <defs>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n    </defs>\n    <g transform=\"translate(168.182381 18.980108)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"164.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"203.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"264.253906\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"292.037109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"353.560547\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"416.939453\" xlink:href=\"#DejaVuSans-109\"/>\n    </g>\n   </g>\n   <g id=\"text_27\">\n    <!-- iraq -->\n    <defs>\n     <path d=\"M 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\nM 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nL 54.390625 -20.796875 \nL 45.40625 -20.796875 \nz\n\" id=\"DejaVuSans-113\"/>\n    </defs>\n    <g transform=\"translate(51.683807 54.815695)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"68.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"130.175781\" xlink:href=\"#DejaVuSans-113\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pabf69199f3\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOOOOOOO"
      ],
      "metadata": {
        "id": "4HVTaEqKRDck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQg7l284V0oa"
      },
      "source": [
        "# Part 3. Prediction-based word vectors (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNq-K6UzzDP"
      },
      "source": [
        "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). If you're feeling adventurous, challenge yourself and try reading GloVe's original paper.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory. Note: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqq7A2IWz011"
      },
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
        "    return wv_from_bin"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYbJ59Jiz7OA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda21452-5a3b-4830-b733-22bd2dfbb254"
      },
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take a couple minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edzctdyh0rDm"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbGSRVPxjaT_"
      },
      "source": [
        "## Reducing dimensionality of Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXr1zGXSjddn"
      },
      "source": [
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead. Run the following cells to:\n",
        "\n",
        "Put 10000 Glove vectors into a matrix M\n",
        "Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_xj1ApzjfOr"
      },
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']):\n",
        "    \"\"\" Put the GloVe vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 200) containing the vectors\n",
        "            word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(224)\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        if w in words:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHVTZLiBjhN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c288523-d21e-42bf-c704-70e23d85d073"
      },
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling words ...\n",
            "Putting 10000 words into word2ind and matrix M...\n",
            "Done.\n",
            "Running Truncated SVD over 10010 words...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoZKWxijmHk"
      },
      "source": [
        "### Question 3.1: GloVe Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDIugqjjo3R"
      },
      "source": [
        "Run the cell below to plot the 2D GloVe embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK438bCgjm98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d4215461-7913-4e2f-cdc8-1861cb0bcd5b"
      },
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "plot_embeddings(M_reduced_normalized, word2ind, words)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 396.792363 248.518125\" width=\"396.792363pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 396.792363 248.518125 \nL 396.792363 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"meee0ffd7da\" style=\"stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\"/>\n    </defs>\n    <g clip-path=\"url(#p3c5c71a9d4)\">\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"119.779247\" xlink:href=\"#meee0ffd7da\" y=\"80.785088\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"45.321307\" xlink:href=\"#meee0ffd7da\" y=\"214.756364\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"309.883665\" xlink:href=\"#meee0ffd7da\" y=\"18.430312\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"349.684943\" xlink:href=\"#meee0ffd7da\" y=\"17.090461\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"348.631426\" xlink:href=\"#meee0ffd7da\" y=\"17.083636\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"241.030262\" xlink:href=\"#meee0ffd7da\" y=\"28.832624\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"276.865963\" xlink:href=\"#meee0ffd7da\" y=\"22.083711\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"172.733521\" xlink:href=\"#meee0ffd7da\" y=\"51.206327\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"297.443512\" xlink:href=\"#meee0ffd7da\" y=\"19.530145\"/>\n     <use style=\"fill:#7e1e9c;fill-opacity:0.5;stroke:#7e1e9c;stroke-opacity:0.5;stroke-width:10;\" x=\"303.107529\" xlink:href=\"#meee0ffd7da\" y=\"18.988521\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0f9290b0ce\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.866976\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(32.72557 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.096974\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(92.955567 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.326971\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(153.185565 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.556969\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(213.415562 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.786966\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(273.64556 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.016964\" xlink:href=\"#m0f9290b0ce\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.0 -->\n      <g transform=\"translate(338.065401 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9e4ee699cb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"204.466872\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 208.266091)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"166.988811\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 170.78803)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"129.51075\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 133.309969)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"92.032689\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 95.831907)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"54.554627\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 58.353846)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9e4ee699cb\" y=\"17.076566\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 20.875785)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- barrels -->\n    <defs>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n    </defs>\n    <g transform=\"translate(119.779247 80.785088)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"124.755859\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"164.119141\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"202.982422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"264.505859\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"292.289062\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"text_14\">\n    <!-- bpd -->\n    <defs>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n    </defs>\n    <g transform=\"translate(45.321307 214.756364)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"126.953125\" xlink:href=\"#DejaVuSans-100\"/>\n    </g>\n   </g>\n   <g id=\"text_15\">\n    <!-- ecuador -->\n    <defs>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n    </defs>\n    <g transform=\"translate(309.883665 18.430312)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"61.523438\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"116.503906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"179.882812\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"241.162109\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"304.638672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.820312\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n   <g id=\"text_16\">\n    <!-- energy -->\n    <defs>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n    </defs>\n    <g transform=\"translate(349.684943 17.090461)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"61.523438\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.902344\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"186.425781\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"225.789062\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"289.265625\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"text_17\">\n    <!-- industry -->\n    <defs>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(348.631426 17.083636)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"91.162109\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"218.017578\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"270.117188\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"309.326172\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"350.439453\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"text_18\">\n    <!-- kuwait -->\n    <defs>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n    </defs>\n    <g transform=\"translate(241.030262 28.832624)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"54.785156\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"118.164062\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"199.951172\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"261.230469\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"289.013672\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n   <g id=\"text_19\">\n    <!-- oil -->\n    <g transform=\"translate(276.865963 22.083711)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"61.181641\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"88.964844\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n   <g id=\"text_20\">\n    <!-- output -->\n    <g transform=\"translate(172.733521 51.206327)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"61.181641\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"124.560547\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"163.769531\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"227.246094\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"290.625\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n   <g id=\"text_21\">\n    <!-- petroleum -->\n    <defs>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n    </defs>\n    <g transform=\"translate(297.443512 19.530145)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"164.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"203.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"264.253906\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"292.037109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"353.560547\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"416.939453\" xlink:href=\"#DejaVuSans-109\"/>\n    </g>\n   </g>\n   <g id=\"text_22\">\n    <!-- iraq -->\n    <defs>\n     <path d=\"M 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\nM 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nL 54.390625 -20.796875 \nL 45.40625 -20.796875 \nz\n\" id=\"DejaVuSans-113\"/>\n    </defs>\n    <g transform=\"translate(303.107529 18.988521)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"27.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"68.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"130.175781\" xlink:href=\"#DejaVuSans-113\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3c5c71a9d4\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cCl0DQjuJN"
      },
      "source": [
        "Write your answer here.\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHZ2o-Ljwwm"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBZbtLYzj1R_"
      },
      "source": [
        "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
        "\n",
        "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLE4DOTNj69N"
      },
      "source": [
        "### Question 3.2: Words with Multiple Meanings (1.5 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LT0bH4kolN"
      },
      "source": [
        "Polysemes and homonyms are words that have more than one meaning (see this wiki page to learn more about the difference between polysemes and homonyms ). Find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain one of the meanings of the words)?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the GenSim documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgoE4V3rj76o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3199bb6b-dd7a-4eaf-c9ca-c032b53b7d60"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    # pprint.pprint(wv_from_bin.most_similar('sound')) # Example of failing one\n",
        "    pprint.pprint(wv_from_bin.most_similar('express'))\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('expressing', 0.5767321586608887),\n",
            " ('expressed', 0.5290095210075378),\n",
            " ('train', 0.5205782651901245),\n",
            " ('sympathy', 0.5139451622962952),\n",
            " ('trains', 0.5050660371780396),\n",
            " ('expresses', 0.49935826659202576),\n",
            " ('bus', 0.4954938292503357),\n",
            " ('regret', 0.49006834626197815),\n",
            " ('sorrow', 0.4864082932472229),\n",
            " ('travel', 0.47741514444351196)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z90e-p_jktYD"
      },
      "source": [
        "Answer: \n",
        "\n",
        "It is most probable that since the explored texts are published articles, most polysemous words are used in a certain sense and usually would not appear in both senses. For example '_sound_' which both means safe and a noise is only found as its noise definition. Although, in the case of express we can see that both the speed and transport definitions appear in the top 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBt1A0Y8kyqx"
      },
      "source": [
        "### Question 3.3: Synonyms & Antonyms (2 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gWr_Cvk3Tu"
      },
      "source": [
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words  (w1,w2,w3)  where  w1  and  w2  are synonyms and  w1  and  w3  are antonyms, but Cosine Distance  (w1,w3)<  Cosine Distance  (w1,w2) .\n",
        "\n",
        "As an example,  w1 =\"happy\" is closer to  w3 =\"sad\" than to  w2 =\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwHp4noktJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09e3032-f51d-48ad-d15e-b707dbfdf668"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "    w1 = 'hello'\n",
        "    w2 = 'hi'\n",
        "    w3 = 'bye'\n",
        "\n",
        "    cos_w1_w2 = wv_from_bin.distance(w1, w2)\n",
        "    cos_w1_w3 = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "    print(f'Cosine Distance w1-w2: {cos_w1_w2}')\n",
        "    print(f'Cosine Distance w1-w3: {cos_w1_w3}')\n",
        "\n",
        "    print(f'cosine Distance w1-w3 < w1-w2: {cos_w1_w3 < cos_w1_w2}')\n",
        "    # ------------------"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Distance w1-w2: 0.6250409185886383\n",
            "Cosine Distance w1-w3: 0.5616524815559387\n",
            "cosine Distance w1-w3 < w1-w2: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AD2asvrk7Y7"
      },
      "source": [
        "Answer:\n",
        "\n",
        "The cosine distance between two words is a measure of the distance between the vectors representing those words in a vector space. It is calculated by taking the angle between the vectors and converting it to a value between 0 and 1 using the formula 1 - cos(angle). Cosine distance ranges from 0 to 1, with a value of 0 indicating that the vectors are identical and a value of 1 indicating that the vectors are completely opposite.\n",
        "\n",
        "It is possible for the cosine distance between the words \"hello\" and \"bye\" to be lower than the cosine distance between the words \"hello\" and \"hi,\" even though \"bye\" and \"hello\" are antonyms and \"hi\" and \"hello\" are synonyms, depending on the vectors that are used to represent these words in a vector space.\n",
        "\n",
        "If the vectors for the words \"hello\" and \"hi\" are pointing in similar directions, and the vectors for the words \"hello\" and \"bye\" are also pointing in similar directions, but more similar than the vectors for \"hello\" and \"hi,\" the cosine distance between \"hello\" and \"bye\" will be lower than the cosine distance between \"hello\" and \"hi.\" This could happen if the vectors for \"bye\" and \"hello\" are more similar to each other than the vectors for \"hi\" and \"hello,\" for example.\n",
        "\n",
        "It's worth noting that the exact cosine distance between two words can vary depending on the context in which they are used and the other words that appear in the same context. In some cases, two words that are normally considered very different might have a lower cosine distance if they are used in similar contexts or if there are other words present that make the vectors for those words more similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0uNtlXZlAdy"
      },
      "source": [
        "### Question 3.4: Analogies with Word Vectors [written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKCiSDhlEBf"
      },
      "source": [
        "Word vectors have been shown to sometimes exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the most_similar function from the GenSim documentation. The function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list (while omitting the input words, which are often the most similar; see this paper). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHlsY4kolA6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25eed1f2-54c5-4488-d6e2-7d707e8d7d2e"
      },
      "source": [
        "# Run this cell to answer the analogy -- man : king :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6978678703308105),\n",
            " ('princess', 0.6081745028495789),\n",
            " ('monarch', 0.5889754891395569),\n",
            " ('throne', 0.5775108933448792),\n",
            " ('prince', 0.5750998854637146),\n",
            " ('elizabeth', 0.546359658241272),\n",
            " ('daughter', 0.5399125814437866),\n",
            " ('kingdom', 0.5318052768707275),\n",
            " ('mother', 0.5168544054031372),\n",
            " ('crown', 0.5164472460746765)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv1gqPl5nQ4H"
      },
      "source": [
        "Let  m ,  k ,  w , and  x  denote the word vectors for man, king, woman, and the answer, respectively. Using only vectors  m ,  k ,  w , and the vector arithmetic operators  +  and    in your answer, what is the expression in which we are maximizing cosine similarity with  x ?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would man and woman lie in the coordinate plane relative to king and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zvj-YaOnTAY"
      },
      "source": [
        "Answer:\n",
        "\n",
        "x = k - m + w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzhOOfnuql"
      },
      "source": [
        "### Question 3.5: Finding Analogies [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5xFEsKVnzT0"
      },
      "source": [
        "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "Note: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrWfGGznRMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2917e2-733b-4801-e93c-b9b7fe75a9b0"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    # Run this cell to answer the analogy -- man : prince :: woman : x\n",
        "    print('First analogy: \"man : prince :: woman : x\":')\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'prince'], negative=['man']))\n",
        "\n",
        "    # Run this cell to answer the analogy -- girl : sister :: boy : x\n",
        "    print('\\nSecond Analogy: \"girl : sister :: boy : x\": ')\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['boy', 'sister'], negative=['girl']))\n",
        "    # ------------------"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First analogy: \"man : prince :: woman : x\":\n",
            "[('princess', 0.7453499436378479),\n",
            " ('duchess', 0.6067375540733337),\n",
            " ('daughter', 0.5600142478942871),\n",
            " ('queen', 0.5452842712402344),\n",
            " ('hrh', 0.5299034118652344),\n",
            " ('wife', 0.5115962028503418),\n",
            " ('marry', 0.5082696676254272),\n",
            " ('naruhito', 0.5037658214569092),\n",
            " ('mistress', 0.5026963949203491),\n",
            " ('crown', 0.4981675446033478)]\n",
            "\n",
            "Second Analogy: \"girl : sister :: boy : x\": \n",
            "[('brother', 0.7562326192855835),\n",
            " ('father', 0.7254698276519775),\n",
            " ('mother', 0.7100989818572998),\n",
            " ('son', 0.7058944702148438),\n",
            " ('cousin', 0.7000033855438232),\n",
            " ('daughter', 0.6871263384819031),\n",
            " ('uncle', 0.6804952025413513),\n",
            " ('siblings', 0.677232027053833),\n",
            " ('elder', 0.6522819995880127),\n",
            " ('nephew', 0.6461338996887207)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygUqza7n8mn"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Both `man : prince :: woman : x` and  `girl : sister :: boy : x` correctly identify the analogies. We thus have the feminine version of `prince` being `princess` and the masculine version of `sister` being `brother`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbSmc52n-Ni"
      },
      "source": [
        "### Question 3.6: Incorrect Analogy [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKhHs5uooAaD"
      },
      "source": [
        "Find an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayAfn_MPnzrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b7f41c-2290-4a21-c0c2-d8ae0252fb16"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "    # Run this cell to answer the analogy -- woman : wife :: man : b \n",
        "    # --> b should be husband\n",
        "    print('First analogy: \"woman : wife :: man : b\":')\n",
        "    missed_1 = wv_from_bin.most_similar(positive=['man', 'wife'], negative=['woman'])\n",
        "    pprint.pprint(missed_1)\n",
        "    print(f'Got \"{missed_1[0][0]}\" when expected \"husband\".')\n",
        "\n",
        "    # Run this cell to answer the analogy -- girl : brother :: boy : b\n",
        "    # --> b should be sister\n",
        "    print('\\nSecond Analogy: \"girl : brother :: boy : b\": ')\n",
        "    missed_2 = wv_from_bin.most_similar(positive=['girl', 'brother'], negative=['boy'])\n",
        "    pprint.pprint(missed_2)\n",
        "    print(f'Got \"{missed_2[0][0]}\" when expected \"sister\".')\n",
        "    # ------------------"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First analogy: \"woman : wife :: man : b\":\n",
            "[('father', 0.7467501163482666),\n",
            " ('brother', 0.745741069316864),\n",
            " ('husband', 0.7437037825584412),\n",
            " ('son', 0.7166001796722412),\n",
            " ('friend', 0.7085943222045898),\n",
            " ('his', 0.6684678792953491),\n",
            " ('cousin', 0.6388828754425049),\n",
            " ('daughter', 0.6335204243659973),\n",
            " ('mother', 0.6283376216888428),\n",
            " ('uncle', 0.6279417276382446)]\n",
            "Got \"father\" when expected \"husband\".\n",
            "\n",
            "Second Analogy: \"girl : brother :: boy : b\": \n",
            "[('daughter', 0.8058238625526428),\n",
            " ('cousin', 0.7796305418014526),\n",
            " ('son', 0.7571232914924622),\n",
            " ('wife', 0.7448716163635254),\n",
            " ('sister', 0.7441308498382568),\n",
            " ('father', 0.7369656562805176),\n",
            " ('niece', 0.7302137613296509),\n",
            " ('nephew', 0.7301906943321228),\n",
            " ('husband', 0.7162861824035645),\n",
            " ('mother', 0.7117223739624023)]\n",
            "Got \"daughter\" when expected \"sister\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqq_EaXoDYl"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Both `woman : wife :: man : b` and  `girl : brother :: boy : b` wrongly identified the analogies. We thus have the feminine version of `brother` being `daughter` instead of `sister` and the masculine version of `wife` being `father` instead of `husband`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBlWGAmoGCy"
      },
      "source": [
        "### Question 3.7: Guided Analysis of Bias in Word Vectors [written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_rVPQteoINq"
      },
      "source": [
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDlvsts2oBsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384a34e0-922f-4bf5-d841-efaccd6218a9"
      },
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
        "# most dissimilar from.\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('employee', 0.6375863552093506),\n",
            " ('workers', 0.6068919897079468),\n",
            " ('nurse', 0.5837947726249695),\n",
            " ('pregnant', 0.5363885164260864),\n",
            " ('mother', 0.5321309566497803),\n",
            " ('employer', 0.5127025842666626),\n",
            " ('teacher', 0.5099576711654663),\n",
            " ('child', 0.5096741914749146),\n",
            " ('homemaker', 0.5019454956054688),\n",
            " ('nurses', 0.4970572590827942)]\n",
            "\n",
            "[('workers', 0.6113258004188538),\n",
            " ('employee', 0.5983108282089233),\n",
            " ('working', 0.5615328550338745),\n",
            " ('laborer', 0.5442320108413696),\n",
            " ('unemployed', 0.5368517637252808),\n",
            " ('job', 0.5278826951980591),\n",
            " ('work', 0.5223963260650635),\n",
            " ('mechanic', 0.5088937282562256),\n",
            " ('worked', 0.505452036857605),\n",
            " ('factory', 0.4940453767776489)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtOxfydoLZb"
      },
      "source": [
        "Answer:\n",
        "\n",
        "The list of female-associated words includes terms such as \"nurse,\" \"mother,\" \"teacher,\" and \"homemaker,\" which are often traditionally considered to be female-dominated occupations or roles. The list of male-associated words includes terms such as \"mechanic\" and \"laborer,\" which are also traditionally considered to be male-dominated occupations or roles.\n",
        "\n",
        "This difference between the two lists reflects gender bias, as it reflects the societal biases and stereotypes that have been encoded in the data used to train the word embeddings. These biases can have harmful consequences, as they can reinforce and perpetuate harmful stereotypes and discriminate against certain groups of people. It is important to be aware of these biases and take steps to mitigate them in any application that uses word embeddings or other language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMsfYJyxoNWT"
      },
      "source": [
        "###  Question 3.8: Independent Analysis of Bias in Word Vectors [code + written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42ajQDcodeV"
      },
      "source": [
        "Use the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9d5cbx3oJsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d288912-8c49-47fb-b6a3-ea81733a73de"
      },
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "    word_to_compare = \"wealthy\"\n",
        "\n",
        "    # Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
        "    # most dissimilar from.\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['woman', word_to_compare], negative=['man']))\n",
        "    print()\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['man', word_to_compare], negative=['woman']))\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('affluent', 0.6492576599121094),\n",
            " ('socialite', 0.5705443024635315),\n",
            " ('middle-class', 0.5464404225349426),\n",
            " ('aristocratic', 0.5330350399017334),\n",
            " ('businesswoman', 0.5314464569091797),\n",
            " ('marry', 0.5257409811019897),\n",
            " ('wealthier', 0.5169167518615723),\n",
            " ('elderly', 0.5112395882606506),\n",
            " ('marrying', 0.510892927646637),\n",
            " ('married', 0.5068483948707581)]\n",
            "\n",
            "[('wealthiest', 0.6119896173477173),\n",
            " ('businessman', 0.5537059307098389),\n",
            " ('rich', 0.5525563955307007),\n",
            " ('businessmen', 0.5454820394515991),\n",
            " ('affluent', 0.5427639484405518),\n",
            " ('richest', 0.5321693420410156),\n",
            " ('tycoons', 0.5197821855545044),\n",
            " ('billionaire', 0.5065683126449585),\n",
            " ('millionaire', 0.49499309062957764),\n",
            " ('fortune', 0.4918596148490906)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBxYCeEolk5"
      },
      "source": [
        "Answer:\n",
        "\n",
        "The list of female-associated words includes terms such as `marry` and `marrying`, `married` ,which are often traditionally associated with wealthy women. Supposing that woman cannot make their own money and have to rely on men which is completely false. The list of male-associated words includes terms such as `businessman`, `tycoons`, `affluent`, and `billionaire`, which are also traditionally associated with men and wealth. But this time putting glory on the men again. This is hugely biased because a man could be a `son` or a `husband` as much as a woman can be a `businesswoman` or `affluent` of her own right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNaetQ4donRi"
      },
      "source": [
        "### Question 3.9: Thinking About Bias [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mktU3tdqtzp"
      },
      "source": [
        "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?\n",
        "\n",
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "One way that bias can get into word vectors is through the data that is used to train the word embedding model. If the data used to train the model reflects societal biases and stereotypes, then the word vectors produced by the model may also reflect these biases.\n",
        "\n",
        "One experiment that could be used to test for or measure this source of bias is to evaluate the word vectors on a set of benchmarks specifically designed to measure bias. There are several such benchmarks available, such as the Word Embedding Association Test (WEAT) and the Word Embedding Association Test for Gender (WEAT-G). These benchmarks allows one to quantitatively measure the degree of bias present in a given set of word vectors by comparing the similarity between pairs of words that are related to a particular bias (e.g. gender, race, sexual orientation, etc.) against the similarity between pairs of words that are not related to that bias.\n",
        "\n",
        "Another way to test for or measure bias in word vectors is to manually examine the most similar words for a given word and see if the resulting list reflects societal biases and stereotypes. For example, if we find that the most similar words for the word \"woman\" include terms such as \"nurse,\" \"homemaker,\" and \"secretary,\" while the most similar words for the word \"man\" include terms such as \"businessman,\" \"engineer,\" and \"doctor,\" this may indicate that the word vectors are reflecting societal biases and stereotypes about the roles and occupations of men and women, which is what we have observed in this practical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-1YihXSxubeS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXQ2d7OsmLl"
      },
      "source": [
        "# Part 4. Prediction-based sentence vectors (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOP9j0mV2rvU"
      },
      "source": [
        "Sentence embeddings are a more powerful representation than word embeddings. They allow you to have out-of-the-box sentence representation of sequences of tokens which is closer to what you would have in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3pVRQ8wE4P"
      },
      "source": [
        "### Question 4.1: How would you represent a sentence with Glove? What are the limits of your proposed implementation? [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1w30iYj0YJn"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2n3vp8g3AEe"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9F_tbMr3CZT"
      },
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBTJnO606Tpc"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnidR9Gg2697"
      },
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load SentenceBERT Vectors\n",
        "        Return:\n",
        "            embedder: sentence embedder \n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return embedder"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzL9oBS4Zoc"
      },
      "source": [
        "%%capture\n",
        "embedder = load_embedding_model()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKkFzRHe6bEm"
      },
      "source": [
        "Inspired by the above, choose the appropriate way to plot the below clusters. Do they make sense to you? What would you improve to get a meaningful plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-o3T8Wz-Feb"
      },
      "source": [
        "### Question 4.2. Evaluate clustering quality of SentenceBERT. What makes it good at clustering sentences? Which method of the two below would you go for? [written] (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqHOmWtqyNo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aaccd1-c69a-457d-f327-20aed54c83b4"
      },
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Perform kmean clustering\n",
        "num_clusters = 5\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  1\n",
            "['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n",
            "\n",
            "Cluster  2\n",
            "['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n",
            "\n",
            "Cluster  3\n",
            "['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n",
            "\n",
            "Cluster  4\n",
            "['The girl is carrying a baby.', 'The baby is carried by the woman']\n",
            "\n",
            "Cluster  5\n",
            "['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4wF5uTy50Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71eaadd-7923-483c-ff84-c884d8ba200e"
      },
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  1\n",
            "['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n",
            "\n",
            "Cluster  5\n",
            "['The girl is carrying a baby.', 'The baby is carried by the woman']\n",
            "\n",
            "Cluster  2\n",
            "['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n",
            "\n",
            "Cluster  3\n",
            "['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n",
            "\n",
            "Cluster  4\n",
            "['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44mDLKmVyOUu"
      },
      "source": [
        "### Question 4.3: SentenceBERT Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ykELvWt7ZX"
      },
      "source": [
        "Plot the above corpus with your favorite method in a 2-dimensional space. Comment on the output. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO Modify with actual correct values\n",
        "#words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "#plot_embeddings(M_reduced_normalized, word2ind, words)"
      ],
      "metadata": {
        "id": "Q54DpY_7gXxu"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqSyMyy-bm8"
      },
      "source": [
        "### Question 4.4: Independent Analysis of Bias in Word Vectors [code + written] (4 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZynvZnq-nqk"
      },
      "source": [
        "Select a corpus of interest, or examples of interest and shed light on one source of bias from SentenceBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQyOEVUWvEmI"
      },
      "source": [],
      "execution_count": 99,
      "outputs": []
    }
  ]
}